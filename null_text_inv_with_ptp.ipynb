{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcafe28c-9f59-4d96-b52f-3e3f45a16c3a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Copyright 2022 Google LLC. Double-click for license information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952f9ab-ed6f-4e99-8304-99a3716734b5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2b343b-1c90-4747-a753-71eb7071a289",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Null-text inversion + Editing with Prompt-to-Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b1b6199-9dfe-4055-8a84-66ff4bfa8901",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union, Tuple, List, Callable, Dict\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "import os\n",
    "# os.system(\"pip install torchvision\")\n",
    "\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "import torch.nn.functional as nnf\n",
    "import numpy as np\n",
    "import abc\n",
    "import ptp_utils\n",
    "import seq_aligner\n",
    "import shutil\n",
    "from torch.optim.adam import Adam\n",
    "from PIL import Image\n",
    "from huggingface_hub import login\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6a4bd1-3130-408b-ae2d-a166b9f19cb7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For loading the Stable Diffusion using Diffusers, follow the instuctions https://huggingface.co/blog/stable_diffusion and update MY_TOKEN with your token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7558a4b4-fec6-4bd2-9c8f-139809b1a1a1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
    "# get your account token from https://huggingface.co/settings/tokens\n",
    "MY_TOKEN = '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' # write\n",
    "#MY_TOKEN = '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' # read\n",
    "\n",
    "login(token=MY_TOKEN)\n",
    "\n",
    "LOW_RESOURCE = False \n",
    "NUM_DDIM_STEPS = 50\n",
    "GUIDANCE_SCALE = 7.5\n",
    "MAX_NUM_WORDS = 77\n",
    "device = torch.device('cuda:3') if torch.cuda.is_available() else torch.device('cpu')\n",
    "ldm_stable = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", use_auth_token=MY_TOKEN, scheduler=scheduler).to(device)\n",
    "\n",
    "try:\n",
    "    ldm_stable.disable_xformers_memory_efficient_attention()\n",
    "except AttributeError:\n",
    "    print(\"Attribute disable_xformers_memory_efficient_attention() is missing\")\n",
    "tokenizer = ldm_stable.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01422991-cafe-4cf0-8406-66f052a75d9b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prompt-to-Prompt code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc083590-15de-4216-8d8d-50336f9f1d34",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LocalBlend:\n",
    "    \n",
    "    def get_mask(self, maps, alpha, use_pool):\n",
    "        k = 1\n",
    "        maps = (maps * alpha).sum(-1).mean(1)\n",
    "        if use_pool:\n",
    "            maps = nnf.max_pool2d(maps, (k * 2 + 1, k * 2 +1), (1, 1), padding=(k, k))\n",
    "        mask = nnf.interpolate(maps, size=(x_t.shape[2:]))\n",
    "        mask = mask / mask.max(2, keepdims=True)[0].max(3, keepdims=True)[0]\n",
    "        mask = mask.gt(self.th[1-int(use_pool)])\n",
    "        mask = mask[:1] + mask\n",
    "        return mask\n",
    "    \n",
    "    def __call__(self, x_t, attention_store):\n",
    "        self.counter += 1\n",
    "        if self.counter > self.start_blend:\n",
    "           \n",
    "            maps = attention_store[\"down_cross\"][2:4] + attention_store[\"up_cross\"][:3]\n",
    "            maps = [item.reshape(self.alpha_layers.shape[0], -1, 1, 16, 16, MAX_NUM_WORDS) for item in maps]\n",
    "            maps = torch.cat(maps, dim=1)\n",
    "            mask = self.get_mask(maps, self.alpha_layers, True)\n",
    "            if self.substruct_layers is not None:\n",
    "                maps_sub = ~self.get_mask(maps, self.substruct_layers, False)\n",
    "                mask = mask * maps_sub\n",
    "            mask = mask.float()\n",
    "            x_t = x_t[:1] + mask * (x_t - x_t[:1])\n",
    "        return x_t\n",
    "       \n",
    "    def __init__(self, prompts: List[str], words: [List[List[str]]], substruct_words=None, start_blend=0.2, th=(.3, .3)):\n",
    "        alpha_layers = torch.zeros(len(prompts),  1, 1, 1, 1, MAX_NUM_WORDS)\n",
    "        for i, (prompt, words_) in enumerate(zip(prompts, words)):\n",
    "            if type(words_) is str:\n",
    "                words_ = [words_]\n",
    "            for word in words_:\n",
    "                ind = ptp_utils.get_word_inds(prompt, word, tokenizer)\n",
    "                alpha_layers[i, :, :, :, :, ind] = 1\n",
    "        \n",
    "        if substruct_words is not None:\n",
    "            substruct_layers = torch.zeros(len(prompts),  1, 1, 1, 1, MAX_NUM_WORDS)\n",
    "            for i, (prompt, words_) in enumerate(zip(prompts, substruct_words)):\n",
    "                if type(words_) is str:\n",
    "                    words_ = [words_]\n",
    "                for word in words_:\n",
    "                    ind = ptp_utils.get_word_inds(prompt, word, tokenizer)\n",
    "                    substruct_layers[i, :, :, :, :, ind] = 1\n",
    "            self.substruct_layers = substruct_layers.to(device)\n",
    "        else:\n",
    "            self.substruct_layers = None\n",
    "        self.alpha_layers = alpha_layers.to(device)\n",
    "        self.start_blend = int(start_blend * NUM_DDIM_STEPS)\n",
    "        self.counter = 0 \n",
    "        self.th=th\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "class EmptyControl:\n",
    "    \n",
    "    \n",
    "    def step_callback(self, x_t):\n",
    "        return x_t\n",
    "    \n",
    "    def between_steps(self):\n",
    "        return\n",
    "    \n",
    "    def __call__(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        return attn\n",
    "\n",
    "    \n",
    "class AttentionControl(abc.ABC):\n",
    "    \n",
    "    def step_callback(self, x_t):\n",
    "        return x_t\n",
    "    \n",
    "    def between_steps(self):\n",
    "        return\n",
    "    \n",
    "    @property\n",
    "    def num_uncond_att_layers(self):\n",
    "        return self.num_att_layers if LOW_RESOURCE else 0\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def forward (self, attn, is_cross: bool, place_in_unet: str):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        if self.cur_att_layer >= self.num_uncond_att_layers:\n",
    "            if LOW_RESOURCE:\n",
    "                attn = self.forward(attn, is_cross, place_in_unet)\n",
    "            else:\n",
    "                h = attn.shape[0]\n",
    "                attn[h // 2:] = self.forward(attn[h // 2:], is_cross, place_in_unet)\n",
    "        self.cur_att_layer += 1\n",
    "        if self.cur_att_layer == self.num_att_layers + self.num_uncond_att_layers:\n",
    "            self.cur_att_layer = 0\n",
    "            self.cur_step += 1\n",
    "            self.between_steps()\n",
    "        return attn\n",
    "    \n",
    "    def reset(self):\n",
    "        self.cur_step = 0\n",
    "        self.cur_att_layer = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cur_step = 0\n",
    "        self.num_att_layers = -1\n",
    "        self.cur_att_layer = 0\n",
    "\n",
    "class SpatialReplace(EmptyControl):\n",
    "    \n",
    "    def step_callback(self, x_t):\n",
    "        if self.cur_step < self.stop_inject:\n",
    "            b = x_t.shape[0]\n",
    "            x_t = x_t[:1].expand(b, *x_t.shape[1:])\n",
    "        return x_t\n",
    "\n",
    "    def __init__(self, stop_inject: float):\n",
    "        super(SpatialReplace, self).__init__()\n",
    "        self.stop_inject = int((1 - stop_inject) * NUM_DDIM_STEPS)\n",
    "        \n",
    "\n",
    "class AttentionStore(AttentionControl):\n",
    "\n",
    "    @staticmethod\n",
    "    def get_empty_store():\n",
    "        return {\"down_cross\": [], \"mid_cross\": [], \"up_cross\": [],\n",
    "                \"down_self\": [],  \"mid_self\": [],  \"up_self\": []}\n",
    "\n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        key = f\"{place_in_unet}_{'cross' if is_cross else 'self'}\"\n",
    "        if attn.shape[1] <= 32 ** 2:  # avoid memory overhead\n",
    "            self.step_store[key].append(attn)\n",
    "        return attn\n",
    "\n",
    "    def between_steps(self):\n",
    "        if len(self.attention_store) == 0:\n",
    "            self.attention_store = self.step_store\n",
    "        else:\n",
    "            for key in self.attention_store:\n",
    "                for i in range(len(self.attention_store[key])):\n",
    "                    self.attention_store[key][i] += self.step_store[key][i]\n",
    "        self.step_store = self.get_empty_store()\n",
    "\n",
    "    def get_average_attention(self):\n",
    "        average_attention = {key: [item / self.cur_step for item in self.attention_store[key]] for key in self.attention_store}\n",
    "        return average_attention\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        super(AttentionStore, self).reset()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.attention_store = {}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AttentionStore, self).__init__()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.attention_store = {}\n",
    "\n",
    "        \n",
    "class AttentionControlEdit(AttentionStore, abc.ABC):\n",
    "    \n",
    "    def step_callback(self, x_t):\n",
    "        if self.local_blend is not None:\n",
    "            x_t = self.local_blend(x_t, self.attention_store)\n",
    "        return x_t\n",
    "        \n",
    "    def replace_self_attention(self, attn_base, att_replace, place_in_unet):\n",
    "        if att_replace.shape[2] <= 32 ** 2:\n",
    "            attn_base = attn_base.unsqueeze(0).expand(att_replace.shape[0], *attn_base.shape)\n",
    "            return attn_base\n",
    "        else:\n",
    "            return att_replace\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        super(AttentionControlEdit, self).forward(attn, is_cross, place_in_unet)\n",
    "        if is_cross or (self.num_self_replace[0] <= self.cur_step < self.num_self_replace[1]):\n",
    "            h = attn.shape[0] // (self.batch_size)\n",
    "            attn = attn.reshape(self.batch_size, h, *attn.shape[1:])\n",
    "            attn_base, attn_repalce = attn[0], attn[1:]\n",
    "            if is_cross:\n",
    "                alpha_words = self.cross_replace_alpha[self.cur_step]\n",
    "                attn_repalce_new = self.replace_cross_attention(attn_base, attn_repalce) * alpha_words + (1 - alpha_words) * attn_repalce\n",
    "                attn[1:] = attn_repalce_new\n",
    "            else:\n",
    "                attn[1:] = self.replace_self_attention(attn_base, attn_repalce, place_in_unet)\n",
    "            attn = attn.reshape(self.batch_size * h, *attn.shape[2:])\n",
    "        return attn\n",
    "    \n",
    "    def __init__(self, prompts, num_steps: int,\n",
    "                 cross_replace_steps: Union[float, Tuple[float, float], Dict[str, Tuple[float, float]]],\n",
    "                 self_replace_steps: Union[float, Tuple[float, float]],\n",
    "                 local_blend: Optional[LocalBlend]):\n",
    "        super(AttentionControlEdit, self).__init__()\n",
    "        self.batch_size = len(prompts)\n",
    "        self.cross_replace_alpha = ptp_utils.get_time_words_attention_alpha(prompts, num_steps, cross_replace_steps, tokenizer).to(device)\n",
    "        if type(self_replace_steps) is float:\n",
    "            self_replace_steps = 0, self_replace_steps\n",
    "        self.num_self_replace = int(num_steps * self_replace_steps[0]), int(num_steps * self_replace_steps[1])\n",
    "        self.local_blend = local_blend\n",
    "\n",
    "class AttentionReplace(AttentionControlEdit):\n",
    "\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        return torch.einsum('hpw,bwn->bhpn', attn_base, self.mapper)\n",
    "      \n",
    "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float,\n",
    "                 local_blend: Optional[LocalBlend] = None):\n",
    "        super(AttentionReplace, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
    "        self.mapper = seq_aligner.get_replacement_mapper(prompts, tokenizer).to(device)\n",
    "        \n",
    "\n",
    "class AttentionRefine(AttentionControlEdit):\n",
    "\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        attn_base_replace = attn_base[:, :, self.mapper].permute(2, 0, 1, 3)\n",
    "        attn_replace = attn_base_replace * self.alphas + att_replace * (1 - self.alphas)\n",
    "        # attn_replace = attn_replace / attn_replace.sum(-1, keepdims=True)\n",
    "        return attn_replace\n",
    "\n",
    "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float,\n",
    "                 local_blend: Optional[LocalBlend] = None):\n",
    "        super(AttentionRefine, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
    "        self.mapper, alphas = seq_aligner.get_refinement_mapper(prompts, tokenizer)\n",
    "        self.mapper, alphas = self.mapper.to(device), alphas.to(device)\n",
    "        self.alphas = alphas.reshape(alphas.shape[0], 1, 1, alphas.shape[1])\n",
    "\n",
    "\n",
    "class AttentionReweight(AttentionControlEdit):\n",
    "\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        if self.prev_controller is not None:\n",
    "            attn_base = self.prev_controller.replace_cross_attention(attn_base, att_replace)\n",
    "        attn_replace = attn_base[None, :, :, :] * self.equalizer[:, None, None, :]\n",
    "        # attn_replace = attn_replace / attn_replace.sum(-1, keepdims=True)\n",
    "        return attn_replace\n",
    "\n",
    "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float, equalizer,\n",
    "                local_blend: Optional[LocalBlend] = None, controller: Optional[AttentionControlEdit] = None):\n",
    "        super(AttentionReweight, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
    "        self.equalizer = equalizer.to(device)\n",
    "        self.prev_controller = controller\n",
    "\n",
    "\n",
    "def get_equalizer(text: str, word_select: Union[int, Tuple[int, ...]], values: Union[List[float],\n",
    "                  Tuple[float, ...]]):\n",
    "    if type(word_select) is int or type(word_select) is str:\n",
    "        word_select = (word_select,)\n",
    "    equalizer = torch.ones(1, 77)\n",
    "    \n",
    "    for word, val in zip(word_select, values):\n",
    "        inds = ptp_utils.get_word_inds(text, word, tokenizer)\n",
    "        equalizer[:, inds] = val\n",
    "    return equalizer\n",
    "\n",
    "def aggregate_attention(attention_store: AttentionStore, res: int, from_where: List[str], is_cross: bool, select: int):\n",
    "    out = []\n",
    "    attention_maps = attention_store.get_average_attention()\n",
    "    num_pixels = res ** 2\n",
    "    for location in from_where:\n",
    "        for item in attention_maps[f\"{location}_{'cross' if is_cross else 'self'}\"]:\n",
    "            if item.shape[1] == num_pixels:\n",
    "                cross_maps = item.reshape(len(prompts), -1, res, res, item.shape[-1])[select]\n",
    "                out.append(cross_maps)\n",
    "    out = torch.cat(out, dim=0)\n",
    "    out = out.sum(0) / out.shape[0]\n",
    "    return out.cpu()\n",
    "\n",
    "\n",
    "def make_controller(prompts: List[str], is_replace_controller: bool, cross_replace_steps: Dict[str, float], self_replace_steps: float, blend_words=None, equilizer_params=None) -> AttentionControlEdit:\n",
    "    if blend_words is None:\n",
    "        lb = None\n",
    "    else:\n",
    "        lb = LocalBlend(prompts, blend_words)\n",
    "    if is_replace_controller:\n",
    "        controller = AttentionReplace(prompts, NUM_DDIM_STEPS, cross_replace_steps=cross_replace_steps, self_replace_steps=self_replace_steps, local_blend=lb)\n",
    "    else:\n",
    "        controller = AttentionRefine(prompts, NUM_DDIM_STEPS, cross_replace_steps=cross_replace_steps, self_replace_steps=self_replace_steps, local_blend=lb)\n",
    "    if equilizer_params is not None:\n",
    "        eq = get_equalizer(prompts[1], equilizer_params[\"words\"], equilizer_params[\"values\"])\n",
    "        controller = AttentionReweight(prompts, NUM_DDIM_STEPS, cross_replace_steps=cross_replace_steps,\n",
    "                                       self_replace_steps=self_replace_steps, equalizer=eq, local_blend=lb, controller=controller)\n",
    "    return controller\n",
    "\n",
    "\n",
    "def show_cross_attention(attention_store: AttentionStore, res: int, from_where: List[str], select: int = 0):\n",
    "    tokens = tokenizer.encode(prompts[select])\n",
    "    decoder = tokenizer.decode\n",
    "    attention_maps = aggregate_attention(attention_store, res, from_where, True, select)\n",
    "    images = []\n",
    "    for i in range(len(tokens)):\n",
    "        image = attention_maps[:, :, i]\n",
    "        image = 255 * image / image.max()\n",
    "        image = image.unsqueeze(-1).expand(*image.shape, 3)\n",
    "        image = image.numpy().astype(np.uint8)\n",
    "        image = np.array(Image.fromarray(image).resize((256, 256)))\n",
    "        image = ptp_utils.text_under_image(image, decoder(int(tokens[i])))\n",
    "        images.append(image)\n",
    "    ptp_utils.view_images(np.stack(images, axis=0))\n",
    "    \n",
    "\n",
    "def show_self_attention_comp(attention_store: AttentionStore, res: int, from_where: List[str],\n",
    "                        max_com=10, select: int = 0):\n",
    "    attention_maps = aggregate_attention(attention_store, res, from_where, False, select).numpy().reshape((res ** 2, res ** 2))\n",
    "    u, s, vh = np.linalg.svd(attention_maps - np.mean(attention_maps, axis=1, keepdims=True))\n",
    "    images = []\n",
    "    for i in range(max_com):\n",
    "        image = vh[i].reshape(res, res)\n",
    "        image = image - image.min()\n",
    "        image = 255 * image / image.max()\n",
    "        image = np.repeat(np.expand_dims(image, axis=2), 3, axis=2).astype(np.uint8)\n",
    "        image = Image.fromarray(image).resize((256, 256))\n",
    "        image = np.array(image)\n",
    "        images.append(image)\n",
    "    ptp_utils.view_images(np.concatenate(images, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a914bda0-c191-4db6-b891-101cde74ddaf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Null Text Inversion code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c442992d-8156-4dfc-a2a5-1fbf8bedb4b2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_512(image_path, left=0, right=0, top=0, bottom=0):\n",
    "    if type(image_path) is str:\n",
    "        image = np.array(Image.open(image_path))[:, :, :3]\n",
    "    else:\n",
    "        image = image_path\n",
    "    h, w, c = image.shape\n",
    "    left = min(left, w-1)\n",
    "    right = min(right, w - left - 1)\n",
    "    top = min(top, h - left - 1)\n",
    "    bottom = min(bottom, h - top - 1)\n",
    "    image = image[top:h-bottom, left:w-right]\n",
    "    h, w, c = image.shape\n",
    "    if h < w:\n",
    "        offset = (w - h) // 2\n",
    "        image = image[:, offset:offset + h]\n",
    "    elif w < h:\n",
    "        offset = (h - w) // 2\n",
    "        image = image[offset:offset + w]\n",
    "    image = np.array(Image.fromarray(image).resize((512, 512)))\n",
    "    return image\n",
    "\n",
    "\n",
    "class NullInversion:\n",
    "    \n",
    "    def prev_step(self, model_output: Union[torch.FloatTensor, np.ndarray], timestep: int, sample: Union[torch.FloatTensor, np.ndarray]):\n",
    "        prev_timestep = timestep - self.scheduler.config.num_train_timesteps // self.scheduler.num_inference_steps\n",
    "        alpha_prod_t = self.scheduler.alphas_cumprod[timestep]\n",
    "        alpha_prod_t_prev = self.scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else self.scheduler.final_alpha_cumprod\n",
    "        beta_prod_t = 1 - alpha_prod_t\n",
    "        pred_original_sample = (sample - beta_prod_t ** 0.5 * model_output) / alpha_prod_t ** 0.5\n",
    "        pred_sample_direction = (1 - alpha_prod_t_prev) ** 0.5 * model_output\n",
    "        prev_sample = alpha_prod_t_prev ** 0.5 * pred_original_sample + pred_sample_direction\n",
    "        return prev_sample\n",
    "    \n",
    "    def next_step(self, model_output: Union[torch.FloatTensor, np.ndarray], timestep: int, sample: Union[torch.FloatTensor, np.ndarray]):\n",
    "        timestep, next_timestep = min(timestep - self.scheduler.config.num_train_timesteps // self.scheduler.num_inference_steps, 999), timestep\n",
    "        alpha_prod_t = self.scheduler.alphas_cumprod[timestep] if timestep >= 0 else self.scheduler.final_alpha_cumprod\n",
    "        alpha_prod_t_next = self.scheduler.alphas_cumprod[next_timestep]\n",
    "        beta_prod_t = 1 - alpha_prod_t\n",
    "        next_original_sample = (sample - beta_prod_t ** 0.5 * model_output) / alpha_prod_t ** 0.5\n",
    "        next_sample_direction = (1 - alpha_prod_t_next) ** 0.5 * model_output\n",
    "        next_sample = alpha_prod_t_next ** 0.5 * next_original_sample + next_sample_direction\n",
    "        return next_sample\n",
    "    \n",
    "    def get_noise_pred_single(self, latents, t, context):\n",
    "        noise_pred = self.model.unet(latents, t, encoder_hidden_states=context)[\"sample\"]\n",
    "        return noise_pred\n",
    "\n",
    "    def get_noise_pred(self, latents, t, is_forward=True, context=None):\n",
    "        latents_input = torch.cat([latents] * 2)\n",
    "        if context is None:\n",
    "            context = self.context\n",
    "        guidance_scale = 1 if is_forward else GUIDANCE_SCALE\n",
    "        noise_pred = self.model.unet(latents_input, t, encoder_hidden_states=context)[\"sample\"]\n",
    "        noise_pred_uncond, noise_prediction_text = noise_pred.chunk(2)\n",
    "        noise_pred = noise_pred_uncond + guidance_scale * (noise_prediction_text - noise_pred_uncond)\n",
    "        if is_forward:\n",
    "            latents = self.next_step(noise_pred, t, latents)\n",
    "        else:\n",
    "            latents = self.prev_step(noise_pred, t, latents)\n",
    "        return latents\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def latent2image(self, latents, return_type='np'):\n",
    "        latents = 1 / 0.18215 * latents.detach()\n",
    "        image = self.model.vae.decode(latents)['sample']\n",
    "        if return_type == 'np':\n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "            image = image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "            image = (image * 255).astype(np.uint8)\n",
    "        return image\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def image2latent(self, image):\n",
    "        with torch.no_grad():\n",
    "            if type(image) is Image:\n",
    "                image = np.array(image)\n",
    "            if type(image) is torch.Tensor and image.dim() == 4:\n",
    "                latents = image\n",
    "            else:\n",
    "                image = torch.from_numpy(image).float() / 127.5 - 1\n",
    "                image = image.permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "                latents = self.model.vae.encode(image)['latent_dist'].mean\n",
    "                latents = latents * 0.18215\n",
    "        return latents\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def init_prompt(self, prompt: str):\n",
    "        uncond_input = self.model.tokenizer(\n",
    "            [\"\"], padding=\"max_length\", max_length=self.model.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        uncond_embeddings = self.model.text_encoder(uncond_input.input_ids.to(self.model.device))[0]\n",
    "        text_input = self.model.tokenizer(\n",
    "            [prompt],\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.model.tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_embeddings = self.model.text_encoder(text_input.input_ids.to(self.model.device))[0]\n",
    "        self.context = torch.cat([uncond_embeddings, text_embeddings])\n",
    "        self.prompt = prompt\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_loop(self, latent):\n",
    "        uncond_embeddings, cond_embeddings = self.context.chunk(2)\n",
    "        all_latent = [latent]\n",
    "        latent = latent.clone().detach()\n",
    "        for i in range(NUM_DDIM_STEPS):\n",
    "            t = self.model.scheduler.timesteps[len(self.model.scheduler.timesteps) - i - 1]\n",
    "            noise_pred = self.get_noise_pred_single(latent, t, cond_embeddings)\n",
    "            latent = self.next_step(noise_pred, t, latent)\n",
    "            all_latent.append(latent)\n",
    "        return all_latent\n",
    "\n",
    "    @property\n",
    "    def scheduler(self):\n",
    "        return self.model.scheduler\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_inversion(self, image):\n",
    "        latent = self.image2latent(image)\n",
    "        image_rec = self.latent2image(latent)\n",
    "        ddim_latents = self.ddim_loop(latent)\n",
    "        return image_rec, ddim_latents\n",
    "\n",
    "    def null_optimization(self, latents, num_inner_steps, epsilon):\n",
    "        uncond_embeddings, cond_embeddings = self.context.chunk(2)\n",
    "        uncond_embeddings_list = []\n",
    "        latent_cur = latents[-1]\n",
    "        bar = tqdm(total=num_inner_steps * NUM_DDIM_STEPS)\n",
    "        for i in range(NUM_DDIM_STEPS):\n",
    "            uncond_embeddings = uncond_embeddings.clone().detach()\n",
    "            uncond_embeddings.requires_grad = True\n",
    "            optimizer = Adam([uncond_embeddings], lr=1e-2 * (1. - i / 100.))\n",
    "            latent_prev = latents[len(latents) - i - 2]\n",
    "            t = self.model.scheduler.timesteps[i]\n",
    "            with torch.no_grad():\n",
    "                noise_pred_cond = self.get_noise_pred_single(latent_cur, t, cond_embeddings)\n",
    "            for j in range(num_inner_steps):\n",
    "                noise_pred_uncond = self.get_noise_pred_single(latent_cur, t, uncond_embeddings)\n",
    "                noise_pred = noise_pred_uncond + GUIDANCE_SCALE * (noise_pred_cond - noise_pred_uncond)\n",
    "                latents_prev_rec = self.prev_step(noise_pred, t, latent_cur)\n",
    "                loss = nnf.mse_loss(latents_prev_rec, latent_prev)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_item = loss.item()\n",
    "                bar.update()\n",
    "                if loss_item < epsilon + i * 2e-5:\n",
    "                    break\n",
    "            for j in range(j + 1, num_inner_steps):\n",
    "                bar.update()\n",
    "            uncond_embeddings_list.append(uncond_embeddings[:1].detach())\n",
    "            with torch.no_grad():\n",
    "                context = torch.cat([uncond_embeddings, cond_embeddings])\n",
    "                latent_cur = self.get_noise_pred(latent_cur, t, False, context)\n",
    "        bar.close()\n",
    "        return uncond_embeddings_list\n",
    "    \n",
    "    def invert(self, image_path: str, prompt: str, offsets=(0,0,0,0), num_inner_steps=10, early_stop_epsilon=1e-5, verbose=False):\n",
    "        self.init_prompt(prompt)\n",
    "        ptp_utils.register_attention_control(self.model, None)\n",
    "        image_gt = load_512(image_path, *offsets)\n",
    "        if verbose:\n",
    "            print(\"DDIM inversion...\")\n",
    "        image_rec, ddim_latents = self.ddim_inversion(image_gt)\n",
    "        if verbose:\n",
    "            print(\"Null-text optimization...\")\n",
    "        uncond_embeddings = self.null_optimization(ddim_latents, num_inner_steps, early_stop_epsilon)\n",
    "        return (image_gt, image_rec), ddim_latents[-1], uncond_embeddings\n",
    "        \n",
    "    \n",
    "    def __init__(self, model):\n",
    "        scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False,\n",
    "                                  set_alpha_to_one=False)\n",
    "        self.model = model\n",
    "        self.tokenizer = self.model.tokenizer\n",
    "        self.model.scheduler.set_timesteps(NUM_DDIM_STEPS)\n",
    "        self.prompt = None\n",
    "        self.context = None\n",
    "\n",
    "null_inversion = NullInversion(ldm_stable)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c919e093-998c-4e4c-92a2-dc9517ef8ea4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Inference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9499145-1a2b-4c91-900e-093c0c08043c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def text2image_ldm_stable(\n",
    "    model,\n",
    "    prompt:  List[str],\n",
    "    controller,\n",
    "    num_inference_steps: int = 50,\n",
    "    guidance_scale: Optional[float] = 7.5,\n",
    "    generator: Optional[torch.Generator] = None,\n",
    "    latent: Optional[torch.FloatTensor] = None,\n",
    "    uncond_embeddings=None,\n",
    "    start_time=50,\n",
    "    return_type='image'\n",
    "):\n",
    "    batch_size = len(prompt)\n",
    "    ptp_utils.register_attention_control(model, controller)\n",
    "    height = width = 512\n",
    "    \n",
    "    text_input = model.tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=model.tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    text_embeddings = model.text_encoder(text_input.input_ids.to(model.device))[0]\n",
    "    max_length = text_input.input_ids.shape[-1]\n",
    "    if uncond_embeddings is None:\n",
    "        uncond_input = model.tokenizer(\n",
    "            [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        uncond_embeddings_ = model.text_encoder(uncond_input.input_ids.to(model.device))[0]\n",
    "    else:\n",
    "        uncond_embeddings_ = None\n",
    "\n",
    "    latent, latents = ptp_utils.init_latent(latent, model, height, width, generator, batch_size)\n",
    "    model.scheduler.set_timesteps(num_inference_steps)\n",
    "    for i, t in enumerate(tqdm(model.scheduler.timesteps[-start_time:])):\n",
    "        if uncond_embeddings_ is None:\n",
    "            context = torch.cat([uncond_embeddings[i].expand(*text_embeddings.shape), text_embeddings])\n",
    "        else:\n",
    "            context = torch.cat([uncond_embeddings_, text_embeddings])\n",
    "        latents = ptp_utils.diffusion_step(model, controller, latents, context, t, guidance_scale, low_resource=False)\n",
    "        \n",
    "    if return_type == 'image':\n",
    "        image = ptp_utils.latent2image(model.vae, latents)\n",
    "    else:\n",
    "        image = latents\n",
    "    return image, latent\n",
    "\n",
    "\n",
    "\n",
    "def run_and_display(prompts, controller, latent=None, run_baseline=False, generator=None, uncond_embeddings=None, verbose=True):\n",
    "    if run_baseline:\n",
    "        print(\"w.o. prompt-to-prompt\")\n",
    "        images, latent = run_and_display(prompts, EmptyControl(), latent=latent, run_baseline=False, generator=generator)\n",
    "        print(\"with prompt-to-prompt\")\n",
    "    images, x_t = text2image_ldm_stable(ldm_stable, prompts, controller, latent=latent, num_inference_steps=NUM_DDIM_STEPS, guidance_scale=GUIDANCE_SCALE, generator=generator, uncond_embeddings=uncond_embeddings)\n",
    "    if verbose:\n",
    "        ptp_utils.view_images(images)\n",
    "    return images, x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233f2e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxillary methods:\n",
    "\n",
    "def save_images(img_number, images, races, prompts, self_replace_steps, \n",
    "                save_perm, perm_saving_dir, save_tmp, ff_input_dir, overwrite_temp_data=False):\n",
    "    \n",
    "    if save_perm:\n",
    "        # save permanentely\n",
    "        num_images = len(prompts)\n",
    "        fig = plt.figure(figsize=(4 * num_images, 4))\n",
    "        columns = num_images\n",
    "        rows = 1\n",
    "        for i in range(1, columns*rows+1):\n",
    "            img = images[i-1]\n",
    "            ax = fig.add_subplot(rows, columns, i)\n",
    "            plt.imshow(img)\n",
    "\n",
    "            for axis in ['top','bottom','left','right']:\n",
    "                ax.spines[axis].set_linewidth(5)\n",
    "                if i == 1:\n",
    "                    ax.spines[axis].set_color(\"red\")\n",
    "                else:\n",
    "                    ax.spines[axis].set_color(\"white\")\n",
    "\n",
    "            plt.tick_params(axis='both', which='both', bottom=False, left=False, labelbottom=False, labelleft = False) \n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        srs_str = str(self_replace_steps).replace(\".\", \"_\")\n",
    "\n",
    "        prompts_lengths = [len(prompt.split(\" \")) for prompt in prompts]\n",
    "        prompts_are_of_equal_length = False # max(prompts_lengths) == min(prompts_lengths)\n",
    "        # print(prompts_are_of_equal_length)\n",
    "        is_equal = \"_eq\" if prompts_are_of_equal_length else \"\"\n",
    "        perm_save_name = f'{perm_saving_dir}/img_{img_number}_{srs_str}{is_equal}.png'\n",
    "        fig.savefig(perm_save_name)\n",
    "        print(f\"Saved images permanentely at '{perm_save_name}'\")\n",
    "\n",
    "\n",
    "    if save_tmp:\n",
    "        # save temporary in fairface folder\n",
    "        if overwrite_temp_data:\n",
    "            os.system(f\"rm -r {ff_input_dir}\")\n",
    "        if not os.path.exists(ff_input_dir):\n",
    "            os.makedirs(ff_input_dir)\n",
    "\n",
    "        size = 300, 300\n",
    "        for i, image_array in enumerate(images):\n",
    "            img = Image.fromarray(image_array)\n",
    "            img.thumbnail(size, Image.LANCZOS)\n",
    "            race, race_index = races[i-1]\n",
    "            race_desc = \"_gt\" if i == 0 else f\"_{race_index}_{race}\"\n",
    "            img.save(f\"{ff_input_dir}/{img_number}{race_desc}{is_equal}.jpg\")\n",
    "        \n",
    "        print(f\"Saved images temporary at '{ff_input_dir}'\")\n",
    "    \n",
    "    if save_perm or save_tmp:\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798e4225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all the same\n",
    "input_dir = \"cropped\"\n",
    "\n",
    "first_image = 2\n",
    "last_image = 50\n",
    "\n",
    "show_race =  [True,      True,       False,      True,              False,               True,           True    ]\n",
    "races =      [\"black\",   \"indian\",   \"latino\",   \"middle eastern\",   \"southeast asian\",  \"east asian\",   \"white\" ]\n",
    "\n",
    "debug_mode = False\n",
    "\n",
    "gender_path = \"fairface/outputs/gender_total.csv\"\n",
    "gender_table = pd.read_csv(gender_path)\n",
    "\n",
    "prompt_prefix = \"a photo of\"\n",
    "\n",
    "save_permanently = True\n",
    "perm_saving_dir = \"outputs/CelebA_auto_gender_cropped\"\n",
    "\n",
    "save_input_ff = True\n",
    "ff_input_dir = \"fairface/raw_input_auto_gender_cropped\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f058d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bb_path = \"./datasets/celeba/list_bbox_celeba.txt\"\n",
    "\n",
    "# # helper method for get_square_detection, source: https://note.nkmk.me/en/python-pillow-add-margin-expand-canvas/\n",
    "# def expand2square(pil_img):\n",
    "#     width, height = pil_img.size\n",
    "#     if width == height:\n",
    "#         return pil_img\n",
    "#     elif width > height:\n",
    "#         result = Image.new(pil_img.mode, (width, width), (0, 0, 0))\n",
    "#         result.paste(pil_img, (0, (width - height) // 2))\n",
    "#         return result\n",
    "#     else:\n",
    "#         result = Image.new(pil_img.mode, (height, height), (0, 0, 0))\n",
    "#         result.paste(pil_img, ((height - width) // 2, 0))\n",
    "#         return result\n",
    "\n",
    "\n",
    "# # get square detection as input for prediction\n",
    "# def get_square_detection(img_nmb, img_path, bboxes):\n",
    "#     bbox = bboxes[img_nmb + 1].split()[1:]\n",
    "#     bbox = [int(value) for value in bbox]\n",
    "\n",
    "#     # get offsets from bbox\n",
    "#     left, top, width, height = bbox\n",
    "#     print(\"left\", left, \"top\", top, \"width\", width, \"height\", height)\n",
    "#     right = left + width\n",
    "#     bottom = top + height\n",
    "#     offsets = (left, top, right, bottom)\n",
    "\n",
    "#     # prepare image for prediction\n",
    "#     img = Image.open(img_path)\n",
    "#     img = img.crop(offsets)\n",
    "#     # print(offsets)\n",
    "#     img.show()\n",
    "#     img.thumbnail((224, 224), Image.Resampling.LANCZOS)\n",
    "#     img = expand2square(img)\n",
    "#     # print(img)\n",
    "#     # img.show()\n",
    "#     return img\n",
    "\n",
    "# # load bounding boxes\n",
    "# bboxes = []\n",
    "# with open(bb_path, \"r\") as f:\n",
    "#     bboxes = f.readlines()\n",
    "\n",
    "# # get face detections to determine gender of the depicted persons \n",
    "# for img_nmb in [1727]: #tqdm(range(1, 202599 + 1)): # 101283: no detection (lying women)\n",
    "#     img_number = f\"{img_nmb:06}\"\n",
    "#     img_path = f\"./CelebA/{img_number}.jpg\"\n",
    "#     face_detection = get_square_detection(img_nmb, img_path, bboxes)\n",
    "#     # face_detection.show()\n",
    "#     # face_detection.save(f\"fairface/detected_faces_CelebA/{img_number}.jpg\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef5cbc6-2581-415f-b608-67f2e87c32f5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for img_nmb in range(first_image, last_image + 1):\n",
    "    img_number = f\"{img_nmb:06}\"\n",
    "    img_path = f\"{input_dir}/{img_number}.jpg\"\n",
    "    print(f\"current image: {img_number}\")\n",
    "\n",
    "    # create original prompt\n",
    "    gender = gender_table.iloc[img_nmb - 1][\"gender\"]\n",
    "    original_prompt = f\"{prompt_prefix} a {gender}\"\n",
    "    prompts = [original_prompt]\n",
    "\n",
    "    if debug_mode:\n",
    "        print(original_prompt)\n",
    "\n",
    "\n",
    "    # do null text inversion\n",
    "    (image_gt, image_enc), x_t, uncond_embeddings = null_inversion.invert(img_path, original_prompt, offsets=(0,0,0,0), verbose=True) # left, right, top, bottom\n",
    "    controller = AttentionStore()\n",
    "    image_inv, x_t = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=uncond_embeddings, verbose=False)\n",
    "\n",
    "    if debug_mode:\n",
    "        print(\"showing from left to right: the ground truth image, the vq-autoencoder reconstruction, the null-text inverted image\")\n",
    "        ptp_utils.view_images([image_gt, image_enc, image_inv[0]])\n",
    "        show_cross_attention(controller, 16, [\"up\", \"down\"])\n",
    "        print(\"\\n\")\n",
    "\n",
    "    \n",
    "    # create prompts for image creation\n",
    "    picked_races = [(race, i)  for i, race in enumerate(races) if show_race[i]]\n",
    "\n",
    "    new_prompts = []\n",
    "    for (race, _) in picked_races:\n",
    "        article = \"an\" if re.match(r\"^(a|e|i|o|u)\", race) else \"a\" \n",
    "        new_prompts.append(f\"{prompt_prefix} {article} {race} {gender}\") #\n",
    "    prompts = [original_prompt] + new_prompts\n",
    "\n",
    "\n",
    "    # show matching races for the created images\n",
    "    # if debug_mode:\n",
    "    #     print(\"original\", end=\"\\t\\t\\t\")\n",
    "    #     for (race, _) in picked_races:\n",
    "    #         print(race, end=\"\\t\\t\\t\")\n",
    "\n",
    "    # create images\n",
    "    print(\"Creating images...\")\n",
    "    cross_replace_steps = {'default_': .8,}\n",
    "    self_replace_steps = 0.6\n",
    "    blend_word = None #((('black',), (\"black\",))) # for local edit. If it is not local yet - use only the source object: blend_word = ((('cat',), (\"cat\",))).\n",
    "    eq_params = None #{\"words\": (\"east asian\",), \"values\": (2,)} # amplify attention to the word \"tiger\" by *2 \n",
    "    controller = None\n",
    "    images = None\n",
    "    image_enc = None\n",
    "    image_gt = None\n",
    "    image_inv = None\n",
    "    prompts_are_of_equal_length = False\n",
    "    controller = make_controller(prompts, prompts_are_of_equal_length, cross_replace_steps, self_replace_steps, blend_word, eq_params)\n",
    "    images, _ = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=uncond_embeddings, verbose=debug_mode)\n",
    "\n",
    "\n",
    "    # save images permanently in separate folder and temporary as input for fairface\n",
    "    save_images(img_number, images, picked_races, prompts, self_replace_steps, \n",
    "                save_permanently, perm_saving_dir, save_input_ff, ff_input_dir, overwrite_temp_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da742d87-b48e-40e1-8b8e-c0f9b7528cc9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # create prompts for image creation\n",
    "# picked_races = [(race, i)  for i, race in enumerate(races) if show_race[i]]\n",
    "\n",
    "# new_prompts = []\n",
    "# for (race, _) in picked_races:\n",
    "#     article = \"an\" if re.match(r\"^(a|e|i|o|u)\", race) else \"a\" \n",
    "#     new_prompts.append(f\"{prompt_prefix} {article} {race} {gender}\") #\n",
    "# prompts = [original_prompt] + new_prompts\n",
    "\n",
    "\n",
    "# # show matching races for the created images\n",
    "# # if debug_mode:\n",
    "# #     print(\"original\", end=\"\\t\\t\\t\")\n",
    "# #     for (race, _) in picked_races:\n",
    "# #         print(race, end=\"\\t\\t\\t\")\n",
    "\n",
    "# # create images\n",
    "# print(\"Creating images...\")\n",
    "# cross_replace_steps = {'default_': .8,}\n",
    "# self_replace_steps = 0.6\n",
    "# blend_word = None #((('black',), (\"black\",))) # for local edit. If it is not local yet - use only the source object: blend_word = ((('cat',), (\"cat\",))).\n",
    "# eq_params = None #{\"words\": (\"east asian\",), \"values\": (2,)} # amplify attention to the word \"tiger\" by *2 \n",
    "# controller = None\n",
    "# images = None\n",
    "# prompts_are_of_equal_length = False\n",
    "# controller = make_controller(prompts, prompts_are_of_equal_length, cross_replace_steps, self_replace_steps, blend_word, eq_params)\n",
    "# images, _ = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=uncond_embeddings, verbose=debug_mode)\n",
    "\n",
    "\n",
    "# # save images permanently in separate folder and temporary as input for fairface\n",
    "# save_images(img_number, images, races, prompts, self_replace_steps, \n",
    "#             save_permanently, perm_saving_dir, save_input_ff, ff_input_dir, overwrite_temp_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b768dd2-a139-4163-823e-15318441ea49",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prompts = [\"a cat sitting next to a mirror\",\n",
    "#            \"a silver cat sculpture sitting next to a mirror\"\n",
    "#         ]\n",
    "\n",
    "# cross_replace_steps = {'default_': .8, }\n",
    "# self_replace_steps = .6\n",
    "# blend_word = ((('cat',), (\"cat\",))) # for local edit\n",
    "# eq_params = {\"words\": (\"silver\", 'sculpture', ), \"values\": (2,2,)}  # amplify attention to the words \"silver\" and \"sculpture\" by *2 \n",
    " \n",
    "# controller = make_controller(prompts, False, cross_replace_steps, self_replace_steps, blend_word, eq_params)\n",
    "# images, _ = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=uncond_embeddings)\n",
    "\n",
    "\n",
    "# prompts = [\"a cat sitting next to a mirror\",\n",
    "#            \"watercolor painting of a cat sitting next to a mirror\"\n",
    "#         ]\n",
    "\n",
    "# cross_replace_steps = {'default_': .8, }\n",
    "# self_replace_steps = .7\n",
    "# blend_word = None\n",
    "# eq_params = {\"words\": (\"watercolor\",  ), \"values\": (5, 2,)}  # amplify attention to the word \"watercolor\" by 5\n",
    " \n",
    "# controller = make_controller(prompts, False, cross_replace_steps, self_replace_steps, blend_word, eq_params)\n",
    "# images, _ = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=uncond_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# reorder images in predefined order (see list of races)\n",
    "# original_image  = images[0]\n",
    "# insert_index = races.index(original_race)\n",
    "# images_reordered = images.copy()\n",
    "\n",
    "# for i in range(1, insert_index + 1):\n",
    "#     images_reordered[i-1] = images[i]\n",
    "# images_reordered[insert_index] = original_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d9987a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 201599/201599 [31:58<00:00, 105.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# # os.system(\"pip install autocrop\")\n",
    "# # from autocrop import Cropper\n",
    "# bb_path = \"./datasets/celeba/list_bbox_celeba.txt\"\n",
    "\n",
    "\n",
    "# bboxes = []\n",
    "# with open(bb_path, \"r\") as f:\n",
    "#     bboxes = f.readlines()\n",
    "\n",
    "\n",
    "# for img_nmb in tqdm(range(1001, 202599 + 1)):\n",
    "#     img_number = f\"{img_nmb:06}\"\n",
    "#     # in dex = 1299\n",
    "\n",
    "#     path = f\"CelebA/uncropped/{img_number}.jpg\"\n",
    "#     # path = f\"datasets/celeba/img_align_celeba/00{index}.jpg\"\n",
    "#     # path = f\"fairface/detected_faces_CelebA/00{index}.jpg\"\n",
    "#     image = Image.open(path)\n",
    "#     # image.show()\n",
    "#     # print(\"size:\" , image.size)\n",
    "\n",
    "#     # get bbox position\n",
    "\n",
    "#     # print(bboxes[img_nmb + 1])\n",
    "#     bbox = bboxes[img_nmb + 1].split()[1:]\n",
    "#     bbox = [int(value) for value in bbox]\n",
    "#     # print(bbox)\n",
    "\n",
    "#     # cropper = Cropper()\n",
    "#     # cropped_array = cropper.crop(path)\n",
    "#     # # if cropped_array:\n",
    "#     # cropped_image = Image.fromarray(cropped_array)\n",
    "#     # cropped_image.show()\n",
    "\n",
    "\n",
    "#     # find centre of bbox\n",
    "#     c_x = bbox[0] + bbox[2] // 2\n",
    "#     c_y = bbox[1] + bbox[3] // 2\n",
    "#     # print(\"center:\", (c_x, c_y))\n",
    "\n",
    "#     if image.width <= image.height:\n",
    "#         crop_size = image.width\n",
    "#         left = 0\n",
    "#         right = crop_size\n",
    "#         top = c_y - crop_size // 2\n",
    "#         bottom = c_y + crop_size // 2\n",
    "\n",
    "#         if top < 0:\n",
    "#             bottom -= top\n",
    "#             top = 0\n",
    "#         if bottom > image.height:\n",
    "#             overlap = bottom - image.height\n",
    "#             top -= overlap\n",
    "#             bottom = image.height\n",
    "\n",
    "#     else:\n",
    "#         crop_size = image.height\n",
    "#         top = 0\n",
    "#         bottom = crop_size\n",
    "#         left = c_x - crop_size // 2\n",
    "#         right = c_x + crop_size // 2\n",
    "\n",
    "#         if left < 0:\n",
    "#             right -= left\n",
    "#             left = 0\n",
    "#         if right > image.width:\n",
    "#             overlap = right - image.width\n",
    "#             left -= overlap\n",
    "#             right = image.width\n",
    "#     # print(crop_size)\n",
    "\n",
    "\n",
    "#     # print(\"bbox\", (left, right, top, bottom))\n",
    "#     image = image.crop((left, top, right, bottom))\n",
    "#     # image.show()\n",
    "#     image.save(f\"CelebA/cropped/{img_number}.jpg\")\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m97",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m97"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
