{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcafe28c-9f59-4d96-b52f-3e3f45a16c3a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Copyright 2022 Google LLC. Double-click for license information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a952f9ab-ed6f-4e99-8304-99a3716734b5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d2b343b-1c90-4747-a753-71eb7071a289",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Editing with Prompt-to-Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b1b6199-9dfe-4055-8a84-66ff4bfa8901",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.system(\"pip install pandas\")\n",
    "from typing import Optional, Union, Tuple, List, Dict\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "import torch.nn.functional as nnf\n",
    "import numpy as np\n",
    "import abc\n",
    "import ptp_utils\n",
    "import seq_aligner\n",
    "from PIL import Image\n",
    "from huggingface_hub import login\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import re\n",
    "from rtpt import RTPT\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "567adf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"CelebA/cropped\"\n",
    "inversion_dir = \"CelebA/latents\"\n",
    "\n",
    "first_image = 8750\n",
    "last_image = 10000\n",
    "\n",
    "# show_race =  [True,      True,       True,       True,              False, False, False   ]\n",
    "show_race =  [False,      False,       True, True,              False, False, False   ]\n",
    "# show_race =  [False, False, False, False,   True,      True,       True]\n",
    "races =      [\"black\",   \"indian\",   \"latino\",   \"middle eastern\",   \"southeast asian\",  \"east asian\",   \"white\" ]\n",
    "\n",
    "debug_mode = False\n",
    "\n",
    "# gender_path = \"fairface/outputs/gender_total.csv\"\n",
    "attr_path = \"datasets/celeba/list_attr_celeba.txt\"\n",
    "genders = pd.read_csv(attr_path, sep=\"\\s+\", skiprows=1)[\"Male\"]\n",
    "genders = genders.replace(-1, \"woman\").replace(1, \"man\").values\n",
    "\n",
    "prompt_prefix = \"a photo of\"\n",
    "\n",
    "save_whole = False\n",
    "whole_saving_dir = \"outputs/CelebA_cropped\"\n",
    "\n",
    "save_solo = True\n",
    "solo_saving_dir = \"CelebA/aug_0_5__0_gender_spec\"\n",
    "\n",
    "device = \"cuda:5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6a4bd1-3130-408b-ae2d-a166b9f19cb7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For loading the Stable Diffusion using Diffusers, follow the instuctions https://huggingface.co/blog/stable_diffusion and update MY_TOKEN with your token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7558a4b4-fec6-4bd2-9c8f-139809b1a1a1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid.\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4a743de7e14d8b94332dffccb2d135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 23 files:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute disable_xformers_memory_efficient_attention() is missing\n"
     ]
    }
   ],
   "source": [
    "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
    "# get your account token from https://huggingface.co/settings/tokens\n",
    "# MY_TOKEN = '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' # write\n",
    "MY_TOKEN = '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' # read\n",
    "login(token=MY_TOKEN)\n",
    "\n",
    "LOW_RESOURCE = False \n",
    "NUM_DDIM_STEPS = 50\n",
    "GUIDANCE_SCALE = 7.5\n",
    "MAX_NUM_WORDS = 77\n",
    "device = torch.device(device) if torch.cuda.is_available() else torch.device('cpu')\n",
    "ldm_stable = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", use_auth_token=MY_TOKEN, scheduler=scheduler).to(device)\n",
    "\n",
    "try:\n",
    "    ldm_stable.disable_xformers_memory_efficient_attention()\n",
    "except AttributeError:\n",
    "    print(\"Attribute disable_xformers_memory_efficient_attention() is missing\")\n",
    "tokenizer = ldm_stable.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01422991-cafe-4cf0-8406-66f052a75d9b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prompt-to-Prompt code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc083590-15de-4216-8d8d-50336f9f1d34",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LocalBlend:\n",
    "    \n",
    "    def get_mask(self, maps, alpha, use_pool):\n",
    "        k = 1\n",
    "        maps = (maps * alpha).sum(-1).mean(1)\n",
    "        if use_pool:\n",
    "            maps = nnf.max_pool2d(maps, (k * 2 + 1, k * 2 +1), (1, 1), padding=(k, k))\n",
    "        mask = nnf.interpolate(maps, size=(x_t.shape[2:]))\n",
    "        mask = mask / mask.max(2, keepdims=True)[0].max(3, keepdims=True)[0]\n",
    "        mask = mask.gt(self.th[1-int(use_pool)])\n",
    "        mask = mask[:1] + mask\n",
    "        return mask\n",
    "    \n",
    "    def __call__(self, x_t, attention_store):\n",
    "        self.counter += 1\n",
    "        if self.counter > self.start_blend:\n",
    "           \n",
    "            maps = attention_store[\"down_cross\"][2:4] + attention_store[\"up_cross\"][:3]\n",
    "            maps = [item.reshape(self.alpha_layers.shape[0], -1, 1, 16, 16, MAX_NUM_WORDS) for item in maps]\n",
    "            maps = torch.cat(maps, dim=1)\n",
    "            mask = self.get_mask(maps, self.alpha_layers, True)\n",
    "            if self.substruct_layers is not None:\n",
    "                maps_sub = ~self.get_mask(maps, self.substruct_layers, False)\n",
    "                mask = mask * maps_sub\n",
    "            mask = mask.float()\n",
    "            x_t = x_t[:1] + mask * (x_t - x_t[:1])\n",
    "        return x_t\n",
    "       \n",
    "    def __init__(self, prompts: List[str], words: [List[List[str]]], substruct_words=None, start_blend=0.2, th=(.3, .3)):\n",
    "        alpha_layers = torch.zeros(len(prompts),  1, 1, 1, 1, MAX_NUM_WORDS)\n",
    "        for i, (prompt, words_) in enumerate(zip(prompts, words)):\n",
    "            if type(words_) is str:\n",
    "                words_ = [words_]\n",
    "            for word in words_:\n",
    "                ind = ptp_utils.get_word_inds(prompt, word, tokenizer)\n",
    "                alpha_layers[i, :, :, :, :, ind] = 1\n",
    "        \n",
    "        if substruct_words is not None:\n",
    "            substruct_layers = torch.zeros(len(prompts),  1, 1, 1, 1, MAX_NUM_WORDS)\n",
    "            for i, (prompt, words_) in enumerate(zip(prompts, substruct_words)):\n",
    "                if type(words_) is str:\n",
    "                    words_ = [words_]\n",
    "                for word in words_:\n",
    "                    ind = ptp_utils.get_word_inds(prompt, word, tokenizer)\n",
    "                    substruct_layers[i, :, :, :, :, ind] = 1\n",
    "            self.substruct_layers = substruct_layers.to(device)\n",
    "        else:\n",
    "            self.substruct_layers = None\n",
    "        self.alpha_layers = alpha_layers.to(device)\n",
    "        self.start_blend = int(start_blend * NUM_DDIM_STEPS)\n",
    "        self.counter = 0 \n",
    "        self.th=th\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "class EmptyControl:\n",
    "    \n",
    "    \n",
    "    def step_callback(self, x_t):\n",
    "        return x_t\n",
    "    \n",
    "    def between_steps(self):\n",
    "        return\n",
    "    \n",
    "    def __call__(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        return attn\n",
    "\n",
    "    \n",
    "class AttentionControl(abc.ABC):\n",
    "    \n",
    "    def step_callback(self, x_t):\n",
    "        return x_t\n",
    "    \n",
    "    def between_steps(self):\n",
    "        return\n",
    "    \n",
    "    @property\n",
    "    def num_uncond_att_layers(self):\n",
    "        return self.num_att_layers if LOW_RESOURCE else 0\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def forward (self, attn, is_cross: bool, place_in_unet: str):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        if self.cur_att_layer >= self.num_uncond_att_layers:\n",
    "            if LOW_RESOURCE:\n",
    "                attn = self.forward(attn, is_cross, place_in_unet)\n",
    "            else:\n",
    "                h = attn.shape[0]\n",
    "                attn[h // 2:] = self.forward(attn[h // 2:], is_cross, place_in_unet)\n",
    "        self.cur_att_layer += 1\n",
    "        if self.cur_att_layer == self.num_att_layers + self.num_uncond_att_layers:\n",
    "            self.cur_att_layer = 0\n",
    "            self.cur_step += 1\n",
    "            self.between_steps()\n",
    "        return attn\n",
    "    \n",
    "    def reset(self):\n",
    "        self.cur_step = 0\n",
    "        self.cur_att_layer = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cur_step = 0\n",
    "        self.num_att_layers = -1\n",
    "        self.cur_att_layer = 0\n",
    "\n",
    "class SpatialReplace(EmptyControl):\n",
    "    \n",
    "    def step_callback(self, x_t):\n",
    "        if self.cur_step < self.stop_inject:\n",
    "            b = x_t.shape[0]\n",
    "            x_t = x_t[:1].expand(b, *x_t.shape[1:])\n",
    "        return x_t\n",
    "\n",
    "    def __init__(self, stop_inject: float):\n",
    "        super(SpatialReplace, self).__init__()\n",
    "        self.stop_inject = int((1 - stop_inject) * NUM_DDIM_STEPS)\n",
    "        \n",
    "\n",
    "class AttentionStore(AttentionControl):\n",
    "\n",
    "    @staticmethod\n",
    "    def get_empty_store():\n",
    "        return {\"down_cross\": [], \"mid_cross\": [], \"up_cross\": [],\n",
    "                \"down_self\": [],  \"mid_self\": [],  \"up_self\": []}\n",
    "\n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        key = f\"{place_in_unet}_{'cross' if is_cross else 'self'}\"\n",
    "        if attn.shape[1] <= 32 ** 2:  # avoid memory overhead\n",
    "            self.step_store[key].append(attn)\n",
    "        return attn\n",
    "\n",
    "    def between_steps(self):\n",
    "        if len(self.attention_store) == 0:\n",
    "            self.attention_store = self.step_store\n",
    "        else:\n",
    "            for key in self.attention_store:\n",
    "                for i in range(len(self.attention_store[key])):\n",
    "                    self.attention_store[key][i] += self.step_store[key][i]\n",
    "        self.step_store = self.get_empty_store()\n",
    "\n",
    "    def get_average_attention(self):\n",
    "        average_attention = {key: [item / self.cur_step for item in self.attention_store[key]] for key in self.attention_store}\n",
    "        return average_attention\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        super(AttentionStore, self).reset()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.attention_store = {}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AttentionStore, self).__init__()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.attention_store = {}\n",
    "\n",
    "        \n",
    "class AttentionControlEdit(AttentionStore, abc.ABC):\n",
    "    \n",
    "    def step_callback(self, x_t):\n",
    "        if self.local_blend is not None:\n",
    "            x_t = self.local_blend(x_t, self.attention_store)\n",
    "        return x_t\n",
    "        \n",
    "    def replace_self_attention(self, attn_base, att_replace, place_in_unet):\n",
    "        if att_replace.shape[2] <= 32 ** 2:\n",
    "            attn_base = attn_base.unsqueeze(0).expand(att_replace.shape[0], *attn_base.shape)\n",
    "            return attn_base\n",
    "        else:\n",
    "            return att_replace\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        super(AttentionControlEdit, self).forward(attn, is_cross, place_in_unet)\n",
    "        if is_cross or (self.num_self_replace[0] <= self.cur_step < self.num_self_replace[1]):\n",
    "            h = attn.shape[0] // (self.batch_size)\n",
    "            attn = attn.reshape(self.batch_size, h, *attn.shape[1:])\n",
    "            attn_base, attn_repalce = attn[0], attn[1:]\n",
    "            if is_cross:\n",
    "                alpha_words = self.cross_replace_alpha[self.cur_step]\n",
    "                attn_repalce_new = self.replace_cross_attention(attn_base, attn_repalce) * alpha_words + (1 - alpha_words) * attn_repalce\n",
    "                attn[1:] = attn_repalce_new\n",
    "            else:\n",
    "                attn[1:] = self.replace_self_attention(attn_base, attn_repalce, place_in_unet)\n",
    "            attn = attn.reshape(self.batch_size * h, *attn.shape[2:])\n",
    "        return attn\n",
    "    \n",
    "    def __init__(self, prompts, num_steps: int,\n",
    "                 cross_replace_steps: Union[float, Tuple[float, float], Dict[str, Tuple[float, float]]],\n",
    "                 self_replace_steps: Union[float, Tuple[float, float]],\n",
    "                 local_blend: Optional[LocalBlend]):\n",
    "        super(AttentionControlEdit, self).__init__()\n",
    "        self.batch_size = len(prompts)\n",
    "        self.cross_replace_alpha = ptp_utils.get_time_words_attention_alpha(prompts, num_steps, cross_replace_steps, tokenizer).to(device)\n",
    "        if type(self_replace_steps) is float:\n",
    "            self_replace_steps = 0, self_replace_steps\n",
    "        self.num_self_replace = int(num_steps * self_replace_steps[0]), int(num_steps * self_replace_steps[1])\n",
    "        self.local_blend = local_blend\n",
    "\n",
    "class AttentionReplace(AttentionControlEdit):\n",
    "\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        return torch.einsum('hpw,bwn->bhpn', attn_base, self.mapper)\n",
    "      \n",
    "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float,\n",
    "                 local_blend: Optional[LocalBlend] = None):\n",
    "        super(AttentionReplace, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
    "        self.mapper = seq_aligner.get_replacement_mapper(prompts, tokenizer).to(device)\n",
    "        \n",
    "\n",
    "class AttentionRefine(AttentionControlEdit):\n",
    "\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        attn_base_replace = attn_base[:, :, self.mapper].permute(2, 0, 1, 3)\n",
    "        attn_replace = attn_base_replace * self.alphas + att_replace * (1 - self.alphas)\n",
    "        # attn_replace = attn_replace / attn_replace.sum(-1, keepdims=True)\n",
    "        return attn_replace\n",
    "\n",
    "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float,\n",
    "                 local_blend: Optional[LocalBlend] = None):\n",
    "        super(AttentionRefine, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
    "        self.mapper, alphas = seq_aligner.get_refinement_mapper(prompts, tokenizer)\n",
    "        self.mapper, alphas = self.mapper.to(device), alphas.to(device)\n",
    "        self.alphas = alphas.reshape(alphas.shape[0], 1, 1, alphas.shape[1])\n",
    "\n",
    "\n",
    "class AttentionReweight(AttentionControlEdit):\n",
    "\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        if self.prev_controller is not None:\n",
    "            attn_base = self.prev_controller.replace_cross_attention(attn_base, att_replace)\n",
    "        attn_replace = attn_base[None, :, :, :] * self.equalizer[:, None, None, :]\n",
    "        # attn_replace = attn_replace / attn_replace.sum(-1, keepdims=True)\n",
    "        return attn_replace\n",
    "\n",
    "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float, equalizer,\n",
    "                local_blend: Optional[LocalBlend] = None, controller: Optional[AttentionControlEdit] = None):\n",
    "        super(AttentionReweight, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
    "        self.equalizer = equalizer.to(device)\n",
    "        self.prev_controller = controller\n",
    "\n",
    "\n",
    "def get_equalizer(text: str, word_select: Union[int, Tuple[int, ...]], values: Union[List[float],\n",
    "                  Tuple[float, ...]]):\n",
    "    if type(word_select) is int or type(word_select) is str:\n",
    "        word_select = (word_select,)\n",
    "    equalizer = torch.ones(1, 77)\n",
    "    \n",
    "    for word, val in zip(word_select, values):\n",
    "        inds = ptp_utils.get_word_inds(text, word, tokenizer)\n",
    "        equalizer[:, inds] = val\n",
    "    return equalizer\n",
    "\n",
    "def aggregate_attention(attention_store: AttentionStore, res: int, from_where: List[str], is_cross: bool, select: int):\n",
    "    out = []\n",
    "    attention_maps = attention_store.get_average_attention()\n",
    "    num_pixels = res ** 2\n",
    "    for location in from_where:\n",
    "        for item in attention_maps[f\"{location}_{'cross' if is_cross else 'self'}\"]:\n",
    "            if item.shape[1] == num_pixels:\n",
    "                cross_maps = item.reshape(len(prompts), -1, res, res, item.shape[-1])[select]\n",
    "                out.append(cross_maps)\n",
    "    out = torch.cat(out, dim=0)\n",
    "    out = out.sum(0) / out.shape[0]\n",
    "    return out.cpu()\n",
    "\n",
    "\n",
    "def make_controller(prompts: List[str], is_replace_controller: bool, cross_replace_steps: Dict[str, float], self_replace_steps: float, blend_words=None, equilizer_params=None) -> AttentionControlEdit:\n",
    "    if blend_words is None:\n",
    "        lb = None\n",
    "    else:\n",
    "        lb = LocalBlend(prompts, blend_words)\n",
    "    if is_replace_controller:\n",
    "        controller = AttentionReplace(prompts, NUM_DDIM_STEPS, cross_replace_steps=cross_replace_steps, self_replace_steps=self_replace_steps, local_blend=lb)\n",
    "    else:\n",
    "        controller = AttentionRefine(prompts, NUM_DDIM_STEPS, cross_replace_steps=cross_replace_steps, self_replace_steps=self_replace_steps, local_blend=lb)\n",
    "    if equilizer_params is not None:\n",
    "        eq = get_equalizer(prompts[1], equilizer_params[\"words\"], equilizer_params[\"values\"])\n",
    "        controller = AttentionReweight(prompts, NUM_DDIM_STEPS, cross_replace_steps=cross_replace_steps,\n",
    "                                       self_replace_steps=self_replace_steps, equalizer=eq, local_blend=lb, controller=controller)\n",
    "    return controller\n",
    "\n",
    "\n",
    "def show_cross_attention(attention_store: AttentionStore, res: int, from_where: List[str], select: int = 0):\n",
    "    tokens = tokenizer.encode(prompts[select])\n",
    "    decoder = tokenizer.decode\n",
    "    attention_maps = aggregate_attention(attention_store, res, from_where, True, select)\n",
    "    images = []\n",
    "    for i in range(len(tokens)):\n",
    "        image = attention_maps[:, :, i]\n",
    "        image = 255 * image / image.max()\n",
    "        image = image.unsqueeze(-1).expand(*image.shape, 3)\n",
    "        image = image.numpy().astype(np.uint8)\n",
    "        image = np.array(Image.fromarray(image).resize((256, 256)))\n",
    "        image = ptp_utils.text_under_image(image, decoder(int(tokens[i])))\n",
    "        images.append(image)\n",
    "    ptp_utils.view_images(np.stack(images, axis=0))\n",
    "    \n",
    "\n",
    "def show_self_attention_comp(attention_store: AttentionStore, res: int, from_where: List[str],\n",
    "                        max_com=10, select: int = 0):\n",
    "    attention_maps = aggregate_attention(attention_store, res, from_where, False, select).numpy().reshape((res ** 2, res ** 2))\n",
    "    u, s, vh = np.linalg.svd(attention_maps - np.mean(attention_maps, axis=1, keepdims=True))\n",
    "    images = []\n",
    "    for i in range(max_com):\n",
    "        image = vh[i].reshape(res, res)\n",
    "        image = image - image.min()\n",
    "        image = 255 * image / image.max()\n",
    "        image = np.repeat(np.expand_dims(image, axis=2), 3, axis=2).astype(np.uint8)\n",
    "        image = Image.fromarray(image).resize((256, 256))\n",
    "        image = np.array(image)\n",
    "        images.append(image)\n",
    "    ptp_utils.view_images(np.concatenate(images, axis=1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c919e093-998c-4e4c-92a2-dc9517ef8ea4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Inference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9499145-1a2b-4c91-900e-093c0c08043c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def text2image_ldm_stable(\n",
    "    model,\n",
    "    prompt:  List[str],\n",
    "    controller,\n",
    "    num_inference_steps: int = 50,\n",
    "    guidance_scale: Optional[float] = 7.5,\n",
    "    generator: Optional[torch.Generator] = None,\n",
    "    latent: Optional[torch.FloatTensor] = None,\n",
    "    uncond_embeddings=None,\n",
    "    start_time=50,\n",
    "    return_type='image'\n",
    "):\n",
    "    batch_size = len(prompt)\n",
    "    ptp_utils.register_attention_control(model, controller)\n",
    "    height = width = 512\n",
    "    \n",
    "    text_input = model.tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=model.tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    text_embeddings = model.text_encoder(text_input.input_ids.to(model.device))[0]\n",
    "    max_length = text_input.input_ids.shape[-1]\n",
    "    if uncond_embeddings is None:\n",
    "        uncond_input = model.tokenizer(\n",
    "            [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        uncond_embeddings_ = model.text_encoder(uncond_input.input_ids.to(model.device))[0]\n",
    "    else:\n",
    "        uncond_embeddings_ = None\n",
    "\n",
    "    latent, latents = ptp_utils.init_latent(latent, model, height, width, generator, batch_size)\n",
    "    model.scheduler.set_timesteps(num_inference_steps)\n",
    "    for i, t in enumerate(tqdm(model.scheduler.timesteps[-start_time:], disable=not debug_mode)):\n",
    "        if uncond_embeddings_ is None:\n",
    "            context = torch.cat([uncond_embeddings[i].expand(*text_embeddings.shape), text_embeddings])\n",
    "        else:\n",
    "            context = torch.cat([uncond_embeddings_, text_embeddings])\n",
    "        latents = ptp_utils.diffusion_step(model, controller, latents, context, t, guidance_scale, low_resource=False)\n",
    "        \n",
    "    if return_type == 'image':\n",
    "        image = ptp_utils.latent2image(model.vae, latents)\n",
    "    else:\n",
    "        image = latents\n",
    "    return image, latent\n",
    "\n",
    "\n",
    "\n",
    "def run_and_display(prompts, controller, latent=None, run_baseline=False, generator=None, uncond_embeddings=None, verbose=True):\n",
    "    if run_baseline:\n",
    "        print(\"w.o. prompt-to-prompt\")\n",
    "        images, latent = run_and_display(prompts, EmptyControl(), latent=latent, run_baseline=False, generator=generator)\n",
    "        print(\"with prompt-to-prompt\")\n",
    "    images, x_t = text2image_ldm_stable(ldm_stable, prompts, controller, latent=latent, num_inference_steps=NUM_DDIM_STEPS, guidance_scale=GUIDANCE_SCALE, generator=generator, uncond_embeddings=uncond_embeddings)\n",
    "    if verbose:\n",
    "        ptp_utils.view_images(images)\n",
    "    return images, x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "233f2e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxillary methods:\n",
    "\n",
    "def save_images(img_number, images, races, prompts, self_replace_steps, \n",
    "                save_whole, whole_saving_dir, save_solo, solo_saving_dir, debug_mode=True, img_size=None):\n",
    "    # prompts_lengths = [len(prompt.split(\" \")) for prompt in prompts]\n",
    "    prompts_are_of_equal_length = False # max(prompts_lengths) == min(prompts_lengths)\n",
    "    is_equal = \"_eq\" if prompts_are_of_equal_length else \"\"\n",
    "    \n",
    "    if save_whole:\n",
    "        # save all images as a whole\n",
    "        num_images = len(prompts)\n",
    "        fig = plt.figure(figsize=(4 * num_images, 4))\n",
    "        columns = num_images\n",
    "        rows = 1\n",
    "        for i in range(1, columns*rows+1):\n",
    "            img = images[i-1]\n",
    "            ax = fig.add_subplot(rows, columns, i)\n",
    "            plt.imshow(img)\n",
    "\n",
    "            for axis in ['top','bottom','left','right']:\n",
    "                ax.spines[axis].set_linewidth(5)\n",
    "                if i == 1:\n",
    "                    ax.spines[axis].set_color(\"red\")\n",
    "                else:\n",
    "                    ax.spines[axis].set_color(\"white\")\n",
    "\n",
    "            plt.tick_params(axis='both', which='both', bottom=False, left=False, labelbottom=False, labelleft = False) \n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        srs_str = str(self_replace_steps).replace(\".\", \"_\")\n",
    "\n",
    "        whole_save_name = f'{whole_saving_dir}/img_{img_number}_{srs_str}{is_equal}.png'\n",
    "        fig.savefig(whole_save_name)\n",
    "        if debug_mode:\n",
    "            print(f\"Saved images as a whole at '{whole_save_name}'\")\n",
    "\n",
    "\n",
    "    if save_solo:\n",
    "        # save images separately\n",
    "        if not os.path.exists(solo_saving_dir):\n",
    "            os.makedirs(solo_saving_dir)\n",
    "\n",
    "        for i, image_array in enumerate(images):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            img = Image.fromarray(image_array)\n",
    "            if img_size is not None:\n",
    "                img.thumbnail(img_size, Image.LANCZOS)\n",
    "            race, race_index = races[i-1]\n",
    "            race_desc = \"_gt\" if i == 0 else f\"_{race_index}_{race}\"\n",
    "            img.save(f\"{solo_saving_dir}/{img_number}{race_desc}{is_equal}.jpg\")\n",
    "        \n",
    "        if debug_mode:\n",
    "            print(f\"Saved images separately at '{solo_saving_dir}'\")\n",
    "    \n",
    "    if debug_mode and (save_whole or save_solo):\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd8cc535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting augmentation...\n",
      "image interval: (60, 60)\n",
      "races used: ['black', 'indian', 'latino', 'middle eastern']\n",
      "device in use: cuda:0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:34<00:00, 34.66s/it]\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting augmentation...\")\n",
    "print(f\"image interval: {(first_image, last_image)}\")\n",
    "print(f\"races used: {list(np.delete(races, ~np.array(show_race)))}\")\n",
    "print(f\"device in use: {device}\\n\")\n",
    "\n",
    "rtpt = RTPT('DR', 'P2P_Dataset_Augmentation', last_image-first_image+1)\n",
    "rtpt.start()\n",
    "for img_nmb in tqdm(range(first_image, last_image + 1)):\n",
    "    img_number = f\"{img_nmb:06}\"\n",
    "    inv_file = img_number + \".pt\"\n",
    "\n",
    "    # load latent vectors, embeddings and other information\n",
    "    data = torch.load(os.path.join(inversion_dir, inv_file), map_location=device)\n",
    "    \n",
    "    \n",
    "    # show_race =  [Tr,     False,      False, False,  True,               True,           True  ]\n",
    "    # random_race = np.random.randint(7)\n",
    "    # random_race = 6\n",
    "    # show_race[random_race] = True\n",
    "    \n",
    "    # controller = AttentionStore()\n",
    "    # if debug_mode:\n",
    "    #     image_inv, latent = run_and_display(prompts, controller, run_baseline=False, latent=latent, uncond_embeddings=uncond_embeddings, verbose=debug_mode)\n",
    "    #     print(\"showing from left to right: the ground truth image, the vq-autoencoder reconstruction, the null-text inverted image\")\n",
    "    #     ptp_utils.view_images([image_gt, image_enc, image_inv[0]])\n",
    "    #     show_cross_attention(controller, 16, [\"up\", \"down\"])\n",
    "    #     print(\"\\n\")\n",
    "\n",
    "    \n",
    "    # create prompts for image creation\n",
    "    original_prompt = data[\"prompt\"]\n",
    "    gender = genders[img_nmb - 1]\n",
    "        \n",
    "    picked_races = [(race, i)  for i, race in enumerate(races) if show_race[i]]\n",
    "    \n",
    "    new_prompts = []\n",
    "    for (race, _) in picked_races:\n",
    "        article = \"an\" if re.match(r\"^(a|e|i|o|u)\", race) else \"a\" \n",
    "        new_prompts.append(f\"{prompt_prefix} {article} {race} {gender}\")\n",
    "    prompts = [original_prompt] + new_prompts\n",
    "\n",
    "\n",
    "    # show matching races for the created images\n",
    "    # if debug_mode:\n",
    "    #     print(\"original\", end=\"\\t\\t\\t\")\n",
    "    #     for (race, _) in picked_races:\n",
    "    #         print(race, end=\"\\t\\t\\t\")\n",
    "\n",
    "    # create images\n",
    "    if debug_mode:\n",
    "        print(\"Creating images...\")\n",
    "    cross_replace_steps = {'default_': .8,}\n",
    "    self_replace_steps = 0.5\n",
    "    blend_word = None #((('black',), (\"black\",))) # for local edit. If it is not local yet - use only the source object: blend_word = ((('cat',), (\"cat\",))).\n",
    "    eq_params = None #{\"words\": (\"white\"), \"values\": (5,)} # amplify attention to the word \"tiger\" by *2 \n",
    "    prompts_are_of_equal_length = False\n",
    "    controller = make_controller(prompts, prompts_are_of_equal_length, cross_replace_steps, self_replace_steps, blend_word, eq_params)\n",
    "\n",
    "    # move embeddings to device\n",
    "    # uncond_emb = []\n",
    "    # for emb in data[\"uncond_embeddings\"]:\n",
    "    #     uncond_emb.append(emb.to(device))\n",
    "\n",
    "    images, _ = run_and_display(prompts, \n",
    "                                controller, \n",
    "                                run_baseline=False, \n",
    "                                latent=data[\"latents\"], \n",
    "                                uncond_embeddings=data[\"uncond_embeddings\"],\n",
    "                                verbose=debug_mode)\n",
    "\n",
    "\n",
    "    # save images as a whole in one folder and separately in another\n",
    "    img_size = (448,448)\n",
    "    save_images(img_number, images, picked_races, prompts, self_replace_steps, \n",
    "                save_whole, whole_saving_dir, save_solo, solo_saving_dir, debug_mode=debug_mode, img_size=img_size)\n",
    "    rtpt.step()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m97",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m97"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
