{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6dc6986490>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# os.system(\"pip install foolbox\")\n",
    "# os.system(\"pip install torchmetrics\")\n",
    "# os.system(\"pip install neptune\")\n",
    "# os.system(\"pip install pandas\")\n",
    "# os.system(\"pip install torchvision\")\n",
    "os.system(\"CUDA_LAUNCH_BLOCKING=1\")\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import neptune\n",
    "from rtpt import RTPT\n",
    "from foolbox import PyTorchModel, accuracy\n",
    "from foolbox.attacks import L2AdditiveGaussianNoiseAttack\n",
    "import eagerpy as ep\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose experiment\n",
    "size_train = 10000 #86744\n",
    "experiments = [\"FairFace\", \"CelebA\", \"CelebA only white\", \"CelebA augmented\"]\n",
    "exp = 1\n",
    "do_attack = True\n",
    "\n",
    "# set celeb paths\n",
    "celeb_attr_path = \"datasets/celeba/list_attr_celeba.txt\"\n",
    "celeb_partitions_path = 'datasets/celeba/list_eval_partition.txt'\n",
    "celeb_race_path = \"CelebA/races/races_ff.csv\"\n",
    "celeb_label_dir = \"CelebA/labels_split/\"\n",
    "celeb_img_dir = \"CelebA/cropped/\"\n",
    "celeb_img_aug_dir = \"CelebA/augmentations/aug_precise_prompts_strong\" #\"CelebA/augmentations/aug_0_5__0/\"\n",
    "celeb_train_csv = f\"train_{size_train}_samples_random.csv\" # \"train_total.csv\"\n",
    "celeb_train_only_white_csv = f\"train_{size_train}_samples_random_white.csv\"\n",
    "celeb_train_aug_csv = \"train_aug_10k_total_fid_harm_precise_prompts.csv\" #\"train_aug_10k_all_samples.csv\"#\"train_aug_10k_distinct_samples_harm.csv\" #f\"train_aug_{size_train}_distinct_samples_correct_gender.csv\"\n",
    "celeb_val_csv = \"val_total.csv\"\n",
    "celeb_test_csv = \"test_total.csv\"\n",
    "\n",
    "\n",
    "# set fairface paths\n",
    "ff_img_dir = \"fairface/dataset/fairface-img-margin125-trainval\"\n",
    "ff_label_dir = \"fairface/dataset/\"\n",
    "ff_train_csv = \"fairface_label_train.csv\"\n",
    "ff_val_csv = \"fairface_label_val.csv\"\n",
    "\n",
    "\n",
    "# set hyperparameters\n",
    "lr = 1e-4\n",
    "num_epochs = 100\n",
    "\n",
    "# Architecture\n",
    "pretrained = False\n",
    "feat_size = (256, 256)\n",
    "bs_train = 128\n",
    "bs_val = 128\n",
    "bs_test = 128\n",
    "device = 'cuda:2'\n",
    "scheduler_step_size = 3\n",
    "scheduler_gamma = 0.9\n",
    "\n",
    "\n",
    "races = [\"Black\", \"Indian\", \"Latino\", \"Middle Eastern\", \"Southeast Asian\", \"East Asian\", \"White\"]\n",
    "# ignored_attributes = [\"Black_Hair\", \"Blond_Hair\", \"Brown_Hair\", \"Pale_Skin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define datasets\n",
    "class CelebaDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading CelebA face images\"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, img_dir, transform=None):\n",
    "    \n",
    "        df = pd.read_csv(csv_path, index_col=None)\n",
    "        # print(df.head())\n",
    "        self.img_dir = img_dir\n",
    "        self.csv_path = csv_path\n",
    "        self.img_names = df[\"Image_Name\"].values\n",
    "        self.races = df[\"Race\"].values\n",
    "        # drop_cols = [\"Image_Name\", \"Race\"] + ignored_attributes\n",
    "        # male = df[\"Male\"].replace(\"Male\", 1).replace(\"Female\", 0)\n",
    "        # if \"Original_Gender\" in df.columns:\n",
    "            # df = df.drop(\"Male\", axis=1).rename(columns={\"Original_Gender\": \"Male\"})\n",
    "            # print(df.columns)\n",
    "        self.y = np.expand_dims(np.array(df[\"Male\"].values), axis=1) #df.drop(drop_cols, axis=1).values #\n",
    "        # self.y = np.expand_dims(np.array(df[\"Male\"].values), axis=1) #df.drop(drop_cols, axis=1).values #\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(os.path.join(self.img_dir,\n",
    "                                      self.img_names[index]))\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = self.y[index]\n",
    "        gt_race = self.races[index]\n",
    "        return img, label, gt_race\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "    \n",
    "\n",
    "\n",
    "class FairFaceDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading FairFace images\"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, img_dir, transform=None):\n",
    "    \n",
    "        df = pd.read_csv(csv_path, index_col=None)\n",
    "        # print(df.head())\n",
    "        self.img_dir = img_dir\n",
    "        self.csv_path = csv_path\n",
    "        self.img_names = df[\"file\"].values\n",
    "        self.races = df[\"race\"].replace(\"Latino_Hispanic\", \"Latino\").values\n",
    "        gender = df[\"gender\"].replace(\"Male\", 1).replace(\"Female\", 0)\n",
    "        self.y = np.expand_dims(np.array(gender.values), axis=1)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(os.path.join(self.img_dir,\n",
    "                                      self.img_names[index]))\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = self.y[index]\n",
    "        gt_race = self.races[index]\n",
    "        return img, label, gt_race\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets based on current experiment\n",
    "num_workers = 6\n",
    "\n",
    "# do normalization here when not performing attack\n",
    "if do_attack:\n",
    "    custom_transform = transforms.Compose([transforms.Resize(feat_size),\n",
    "                                       transforms.ToTensor()])\n",
    "else:\n",
    "    custom_transform = transforms.Compose([transforms.Resize(feat_size),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "# training dataset\n",
    "if experiments[exp].startswith(\"CelebA\"):\n",
    "\n",
    "    if experiments[exp].endswith(\"augmented\"):\n",
    "        train_csv = celeb_train_aug_csv\n",
    "        train_img_dir = celeb_img_aug_dir\n",
    "    \n",
    "    else:\n",
    "        if \"only white\" in experiments[exp]:\n",
    "            train_csv = celeb_train_only_white_csv\n",
    "        else:\n",
    "            train_csv = celeb_train_csv\n",
    "        train_img_dir = celeb_img_dir\n",
    "\n",
    "    train_dataset = CelebaDataset(csv_path=celeb_label_dir + train_csv,\n",
    "                                img_dir=train_img_dir,\n",
    "                                transform=custom_transform)\n",
    "\n",
    "if experiments[exp].startswith(\"FairFace\"):\n",
    "    train_dataset = FairFaceDataset(csv_path=ff_label_dir + ff_train_csv,\n",
    "                                    img_dir=ff_img_dir,\n",
    "                                    transform=custom_transform)\n",
    "\n",
    "\n",
    "# validation dataset\n",
    "val_dataset_celeb = CelebaDataset(csv_path=celeb_label_dir + celeb_val_csv,\n",
    "                            img_dir=celeb_img_dir,\n",
    "                            transform=custom_transform)\n",
    "\n",
    "val_dataset_ff = FairFaceDataset(csv_path=ff_label_dir + ff_val_csv,\n",
    "                                img_dir=ff_img_dir,\n",
    "                                transform=custom_transform)\n",
    "\n",
    "\n",
    "\n",
    "# test datasets\n",
    "test_dataset_celeb = CelebaDataset(csv_path=celeb_label_dir + celeb_test_csv,\n",
    "                            img_dir=celeb_img_dir,\n",
    "                            transform=custom_transform)\n",
    "\n",
    "test_dataset_ff = val_dataset_ff\n",
    "\n",
    "\n",
    "# create dataloaders on these datasets\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=bs_train,\n",
    "                          shuffle=True,\n",
    "                          num_workers=num_workers)\n",
    "\n",
    "val_loader = DataLoader(dataset=val_dataset_ff,\n",
    "                          batch_size=bs_val,\n",
    "                          shuffle=False,\n",
    "                          num_workers=num_workers)\n",
    "\n",
    "test_loader_celeb = DataLoader(dataset=test_dataset_celeb,\n",
    "                          batch_size=bs_test,\n",
    "                          shuffle=False,\n",
    "                          num_workers=num_workers)\n",
    "\n",
    "test_loader_ff = DataLoader(dataset=test_dataset_ff,\n",
    "                          batch_size=bs_test,\n",
    "                          shuffle=False,\n",
    "                          num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://new-ui.neptune.ai/danielritter0508/aug-precise-pre-trained/e/AUGPRE-82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/foolbox/models/pytorch.py:37: UserWarning: The PyTorch model is in training mode and therefore might not be deterministic. Call the eval() method to set it in evaluation mode if this is not intended.\n",
      "  \"The PyTorch model is in training mode and therefore might\"\n"
     ]
    }
   ],
   "source": [
    "# build model, define loss and create optimizer\n",
    "model = resnet34(weights=(ResNet34_Weights.DEFAULT if pretrained else None))\n",
    "\n",
    "\n",
    "model.fc = nn.Sequential(nn.Linear(model.fc.in_features, 256),\n",
    "                         nn.ReLU(inplace=True),\n",
    "                         nn.Linear(256, train_dataset.y.shape[1]))\n",
    "model.to(device)\n",
    "bin_ce = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=scheduler_step_size, gamma=scheduler_gamma)\n",
    "\n",
    "# initialize experiment tracking\n",
    "run = neptune.init_run(\n",
    "    project=\"danielritter0508/aug-precise-pre-trained\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vbmV3LXVpLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9uZXctdWkubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxNDI4OGY4ZC05YmUzLTRjNzAtOTE2NS0xNzU0NTkyMTJiYmUifQ==\",\n",
    ")\n",
    "run[\"params\"] = {\"experiment\": experiments[exp], \"learning_rate\": lr, \"num_epochs\": num_epochs, \n",
    "                 \"scheduler_step\": scheduler_step_size, \"gamma\": scheduler_gamma, \"pretrained\": pretrained}\n",
    "\n",
    "# init foolbox model\n",
    "normalization = dict(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], axis=-3)\n",
    "fool_model = PyTorchModel(model, bounds=(0, 1), device=device)\n",
    "attack = L2AdditiveGaussianNoiseAttack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the root of the mean of the squared differences between the values and their average\n",
    "def get_RMSD(values):\n",
    "    # calculate the average of the values\n",
    "    avg = sum(values) / len(values)\n",
    "\n",
    "    # calculate the sum of the squared deviations\n",
    "    squared_deviations_sum = sum((x - avg) ** 2 for x in values)\n",
    "    \n",
    "    # calculate the mean squared deviation and take the root of it\n",
    "    rmsd = (squared_deviations_sum / len(values)) ** 0.5\n",
    "    return rmsd\n",
    "\n",
    "\n",
    "# calculate all necessary metrics\n",
    "def evaluate_metrics(model, fool_model, data_loader, device, log_description, show_tqdm=False, decimals=4):\n",
    "\n",
    "    # initialize all parameters with zeros\n",
    "    correct_predictions, true_pos, true_neg, pos_preds, pos_targets, num_examples = np.zeros((6, len(races)))\n",
    "    # f_acc = 0\n",
    "    # iterate through validation/test data and collect necessary basic data\n",
    "    for _, (features, targets, gt_races) in tqdm(enumerate(data_loader), total=len(data_loader), desc=\"Evaluating\", disable=not show_tqdm):#\n",
    "       \n",
    "        # predictions = (fool_model(images) >= 0).float32()\n",
    "        # accuracy = (predictions == labels).float32().mean()\n",
    "        # f_acc += accuracy.item()\n",
    "        # print(facc)\n",
    "        # f_acc += facc\n",
    "\n",
    "        predictions = (model(features.to(device)) >= 0).cpu().numpy()\n",
    "        targets = targets.numpy()\n",
    "\n",
    "        # prepape annotated races for race-wise split afterwards\n",
    "        gt_races = np.array([races.index(race) for race in gt_races])\n",
    "        gt_races = np.expand_dims(gt_races, axis=1)\n",
    "\n",
    "        # collect the necessary data split by annotated race\n",
    "        for j in range(len(races)):\n",
    "            correct_preds = (gt_races == j) & (predictions == targets)\n",
    "            true_pos[j] += (correct_preds & (predictions == 1)).sum()\n",
    "            true_neg[j] += (correct_preds & (predictions == 0)).sum()\n",
    "            correct_predictions[j] += correct_preds.sum()\n",
    "            pos_targets[j] += ((gt_races == j) & (targets == 1)).sum()\n",
    "            pos_preds[j] += np.where(gt_races == j, predictions, 0).sum()\n",
    "            num_examples[j] += (gt_races == j).sum()\n",
    "\n",
    "    # f_acc = f_acc / len(data_loader)\n",
    "    # print(f_acc)\n",
    "\n",
    "    # calculate the metrics    \n",
    "    zero = 1e-10\n",
    "\n",
    "    # accuracy\n",
    "    total_acc = correct_predictions.sum() / num_examples.sum()\n",
    "    print(total_acc)\n",
    "    accs = correct_predictions / (num_examples + zero)\n",
    "\n",
    "    # bias (inter-race accuracy variation)\n",
    "    mad = np.log(max(accs) / min(accs))\n",
    "    rmsd_acc = get_RMSD(accs)\n",
    "\n",
    "    # precision\n",
    "    total_prec = true_pos.sum() / (pos_preds.sum() + zero)\n",
    "    precs = true_pos / (pos_preds + zero)\n",
    "\n",
    "    # recall\n",
    "    total_rec = true_pos.sum() / (pos_targets.sum() + zero)\n",
    "    recs = true_pos / (pos_targets + zero)\n",
    "\n",
    "    # F1 score\n",
    "    total_f1 = 2 * total_prec * total_rec / (total_prec + total_rec)\n",
    "    f1_scores = 2 * precs * recs / (precs + recs)\n",
    "\n",
    "\n",
    "    # round and track general results\n",
    "    total_acc, mad, rmsd_acc, total_prec, total_rec, total_f1 = np.round((total_acc, mad, rmsd_acc, total_prec, total_rec, total_f1), decimals)\n",
    "    metrics = {\"total_acc\": total_acc, \"MAD\": mad, \"RMSD\": rmsd_acc, \"total_prec\": total_prec, \"total_rec\": total_rec, \"total_f1\": total_f1}\n",
    "    for (name, value) in metrics.items():\n",
    "        run[log_description + \"/\" + name].append(value)\n",
    "\n",
    "\n",
    "    # round and track race-specific results\n",
    "    accs, precs, recs, f1_scores = np.round((accs, precs, recs, f1_scores), decimals)\n",
    "    for race, acc, prec, rec, f1  in zip(races, accs, precs, recs, f1_scores):\n",
    "        run[log_description + \"/acc/\" + race].append(acc)\n",
    "        run[log_description + \"/prec/\" + race].append(prec)\n",
    "        run[log_description + \"/rec/\" + race].append(rec)\n",
    "        run[log_description + \"/f1/\" + race].append(f1)\n",
    "    \n",
    "    return total_acc, accs, mad, rmsd_acc, total_f1, f1_scores, total_prec, precs, total_rec, recs\n",
    "\n",
    "\n",
    "\n",
    "def get_elapsed_time(start_time):\n",
    "    elapsed = int(time.time() - start_time)\n",
    "    m, s = divmod(elapsed, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return f\"{h}:{m:02d}:{s:02d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating experiment 'CelebA' with a lr of 0.0001 and 10000 samples on device cuda:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 001/100:  13%|█▎        | 10/79 [00:04<00:32,  2.11it/s, loss=0.6109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5287566185868176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 002/100:  13%|█▎        | 10/79 [00:04<00:31,  2.20it/s, loss=0.4852]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_362841/1267247921.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# update model params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    250\u001b[0m                  \u001b[0mfused\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fused'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                  \u001b[0mgrad_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                  found_inf=found_inf)\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    314\u001b[0m          \u001b[0mdifferentiable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdifferentiable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m          \u001b[0mgrad_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m          found_inf=found_inf)\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcapturable\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdifferentiable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "start_time = time.time()\n",
    "\n",
    "rtpt = RTPT('DR', 'Train_Gender_Classifier', num_epochs)\n",
    "rtpt.start()\n",
    "\n",
    "print(f\"Initiating experiment '{experiments[exp]}' with a lr of {lr} and {size_train} samples on device {device}\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {(epoch+1):03d}/{num_epochs:03d}\")\n",
    "    for batch_idx, (features, targets, _) in pbar:\n",
    "        \n",
    "        features = features.to(device)\n",
    "        targets = targets.float().to(device)\n",
    "            \n",
    "        # forward and backward pass\n",
    "        logits = model(features)\n",
    "        # print(logits.shape, targets.shape)\n",
    "        loss = bin_ce(logits, targets)\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "        run[\"train/loss\"].append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # update model params \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if batch_idx == 10:\n",
    "            break\n",
    "        #     first = features[0].cpu().numpy()\n",
    "        #     first = np.moveaxis(first, 0, -1) * 255\n",
    "        #     first = first.astype(np.uint8)\n",
    "        #     image = Image.fromarray(first)\n",
    "        #     print(\"Woman\" if int(targets[0].item()) == 0 else \"Man\")\n",
    "        #     image.show()\n",
    "       \n",
    "            # print(\"Gender:\", \"Man\" if targets)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False): # save memory during inference\n",
    "        evaluate_metrics(model, fool_model, val_loader, device, \"valid\")\n",
    "\n",
    "    scheduler.step()\n",
    "    rtpt.step()\n",
    "    \n",
    "print(f\"Total Training Time: {get_elapsed_time(start_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation of experiment: 'CelebA augmented'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 156/156 [00:28<00:00,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation CelebA test set:\n",
      "Total accuracy: 69.91%\t| Accuracies:\t[0.6824 0.6872 0.673  0.7419 0.6174 0.6843 0.7029]\n",
      "Maximum accuracy disparity: 0.1837\n",
      "Total precision: 56.66%\t| Precisions:\t[0.6469 0.5489 0.4537 0.6642 0.5502 0.5061 0.561 ]\n",
      "Total recall: 94.23%\t| Recalls:\t[0.9711 0.9665 0.9769 0.9492 0.9514 0.9425 0.9326]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: 100%|██████████| 86/86 [00:12<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation FairFace test set:\n",
      "Total accuracy: 58.71%\t| Accuracies:\t[0.5263 0.5561 0.5638 0.708  0.5809 0.5903 0.6048]\n",
      "Maximum accuracy disparity: 0.2965\n",
      "Total precision: 56.60%\t| Precisions:\t[0.5217 0.5292 0.5295 0.708  0.5577 0.5545 0.5858]\n",
      "Total recall: 93.97%\t| Recalls:\t[0.9324 0.9641 0.9634 0.9631 0.9333 0.9292 0.9064]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate experiment on test sets\n",
    "with torch.set_grad_enabled(False): # save memory during inference\n",
    "    print(f\"\\nEvaluation of experiment: '{experiments[exp]}'\\n\")\n",
    "\n",
    "    # evaluation CelebA\n",
    "    total_acc, accs, MAD, rmsd_acc, total_f1, f1_scores, total_prec, precs, total_rec, recs = evaluate_metrics(model, test_loader_celeb, device, \"eval_celeb\", show_tqdm=True)\n",
    "    print(\"\\nEvaluation CelebA test set:\")\n",
    "    print(f\"Total accuracy: {total_acc:.2%}\\t| Accuracies:\\t{accs}\")\n",
    "    print(f\"Maximum accuracy disparity: {MAD}\")\n",
    "    print(f\"Total precision: {total_prec:.2%}\\t| Precisions:\\t{precs}\")\n",
    "    print(f\"Total recall: {total_rec:.2%}\\t| Recalls:\\t{recs}\\n\")\n",
    "\n",
    "    # evaluation FairFace\n",
    "    total_acc, accs, MAD, rmsd_acc, total_f1, f1_scores, total_prec, precs, total_rec, recs = evaluate_metrics(model, test_loader_ff, device, \"eval_ff\", show_tqdm=True)\n",
    "    print(\"\\nEvaluation FairFace test set:\")\n",
    "    print(f\"Total accuracy: {total_acc:.2%}\\t| Accuracies:\\t{accs}\")\n",
    "    print(f\"Maximum accuracy disparity: {MAD}\")\n",
    "    print(f\"Total precision: {total_prec:.2%}\\t| Precisions:\\t{precs}\")\n",
    "    print(f\"Total recall: {total_rec:.2%}\\t| Recalls:\\t{recs}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
