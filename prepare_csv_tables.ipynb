{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_path = \"datasets/celeba/list_attr_celeba.txt\"\n",
    "partitions_path = 'datasets/celeba/list_eval_partition.txt'\n",
    "race_path = \"CelebA/races/races_ff.csv\"\n",
    "label_dir = \"CelebA/labels_split/\"\n",
    "img_celeb_dir = \"CelebA/cropped/\"\n",
    "# img_created_dir = \"CelebA/augmented/raw_input_auto_gender_cropped\"\n",
    "races = [\"Black\", \"Indian\", \"Latino\", \"Middle Eastern\", \"Southeast Asian\", \"East Asian\", \"White\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all labels\n",
    "\n",
    "# all_labels = pd.read_csv(attributes_path, sep=\"\\s+\", skiprows=1).rename_axis(\"Image_Name\").reset_index().replace(-1, 0)\n",
    "# print(all_labels.head())\n",
    "\n",
    "# # create training and validation labels out off all_labels\n",
    "# y_train = all_labels[:size_train]\n",
    "# # y_train.columns = all_labels.columns\n",
    "# print(y_train.shape)\n",
    "\n",
    "# y_train = pd.DataFrame(columns=all_labels.columns)\n",
    "# for i, row in tqdm(all_labels[:size_train].iterrows(), total=size_train):\n",
    "#     img_number = f\"{(i+1):06}\"\n",
    "#     for j, race in enumerate(races):\n",
    "#         new_image_name = f\"{img_number}_{j}_{race.lower()}.jpg\"\n",
    "#         row[0] = new_image_name\n",
    "#         y_train = y_train.append(row)\n",
    "#     # if i == 0:\n",
    "#     #     break\n",
    "# print(y_train.shape)\n",
    "# y_train.to_csv(label_dir + \"created_train2.csv\")\n",
    "\n",
    "# y_val = all_labels[-1000:]\n",
    "# y_val.to_csv(label_dir + \"val.csv\")\n",
    "\n",
    "# y_test = all_labels[-2000:-1000]\n",
    "# y_test.to_csv(label_dir + \"test.csv\")\n",
    "\n",
    "# print(\"y_train shape:\", y_train.shape)\n",
    "# print(\"y_val shape:\", y_val.shape)\n",
    "# print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162770 19867 19962\n"
     ]
    }
   ],
   "source": [
    "df_attr = pd.read_csv(attributes_path, sep=\"\\s+\", skiprows=1)\n",
    "df_attr = df_attr.replace(-1, 0)\n",
    "# df_attr.head()\n",
    "\n",
    "df_part = pd.read_csv(partitions_path, sep=\"\\s+\", skiprows=0, header=None)\n",
    "df_part.columns = ['Image_Name', 'Partition']\n",
    "df_part = df_part.set_index('Image_Name')\n",
    "# print(df_part.head())\n",
    "\n",
    "df_attr_part = df_attr.merge(df_part, left_index=True, right_index=True)\n",
    "df_attr_part.index = df_attr_part.index.rename('Image_Name')\n",
    "# print(df_attr_part.columns)\n",
    "\n",
    "df_race = pd.read_csv(race_path, usecols=[\"face_name_align\", \"race\"])\n",
    "df_race.columns = ['Image_Name', 'Race']\n",
    "df_race = df_race.set_index('Image_Name')\n",
    "# print(df_race.head())\n",
    "\n",
    "df_total = df_attr_part.merge(df_race, left_index=True, right_index=True)\n",
    "# df_total.head()\n",
    "\n",
    "\n",
    "# realize standard celeba split\n",
    "train_set = df_total.loc[df_total['Partition'] == 0].drop(\"Partition\", axis=1)\n",
    "val_set = df_total.loc[df_total['Partition'] == 1].drop(\"Partition\", axis=1)\n",
    "test_set = df_total.loc[df_total['Partition'] == 2].drop(\"Partition\", axis=1)\n",
    "\n",
    "print(len(train_set), len(val_set), len(test_set))\n",
    "# train_set.to_csv(label_dir + 'train_total.csv')\n",
    "# val_set.to_csv(label_dir + 'val_total.csv')\n",
    "# test_set.to_csv(label_dir + 'test_total.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training labels for augmented data (expecting leading sample)\n",
    "# size_augmented_data = 1000\n",
    "# df_attr_prepared = df_attr[:size_augmented_data].rename_axis(\"Image_Name\").reset_index()\n",
    "# train_set_aug = pd.DataFrame(columns=df_attr_prepared.columns)\n",
    "# # print(train_set_aug.head())\n",
    "# for i, row in tqdm(df_attr_prepared.iterrows(), total=size_augmented_data):\n",
    "#     img_number = f\"{(i+1):06}\"\n",
    "#     for j, race in enumerate(races):\n",
    "#         new_image_name = f\"{img_number}_{j}_{race.lower()}.jpg\"\n",
    "#         row[0] = new_image_name\n",
    "#         row[\"Race\"] = race\n",
    "#         train_set_aug = train_set_aug.append(row)\n",
    "# train_set_aug = train_set_aug.set_index(\"Image_Name\")\n",
    "# print(train_set_aug.shape)\n",
    "# train_set_aug.to_csv(label_dir + f\"train_aug_{size_augmented_data}_samples.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split validation set by race\n",
    "# grouped_df = val_set.groupby(val_set.Race)\n",
    "# val_races = []\n",
    "# print(f\"Validation set ({len(val_set)} images):\")\n",
    "# for race in races:\n",
    "#       val_race = grouped_df.get_group(race)\n",
    "#       print(f\"{race}: {len(val_race)}\", end=\"\\t\")\n",
    "#       val_race.to_csv(label_dir+\"val/\"+race+\".csv\")\n",
    "# print(\"\\n\")\n",
    "\n",
    "# # split test set by race\n",
    "# grouped_df = test_set.groupby(test_set.Race)\n",
    "# test_races = []\n",
    "# print(f\"Test set ({len(val_set)} images):\")\n",
    "# for race in races:\n",
    "#       test_race = grouped_df.get_group(race)\n",
    "#       print(f\"{race}: {len(test_race)}\", end=\"\\t\")\n",
    "#       test_race.to_csv(label_dir+\"test/\"+race+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take random train sample\n",
    "# size_sample = 500\n",
    "# rand_perm = np.random.permutation(len(train_set))\n",
    "# sample_set = train_set.iloc[rand_perm[:size_sample]]\n",
    "# sample_set.to_csv(label_dir + f'train_{size_sample}_samples_shuffled.csv')\n",
    "# print(sample_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # take leading train sample\n",
    "# size_sample = 1000\n",
    "# only_white = train_set[train_set[\"Race\"] == \"White\"]\n",
    "# sample_set = only_white[:size_sample]\n",
    "# sample_set.to_csv(label_dir + f'train_{size_sample}_samples_only_white.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take random train sample\n",
    "# size_sample = 10000\n",
    "# train_sample = train_set.sample(n=size_sample)\n",
    "# train_sample.to_csv(label_dir + f'train_{size_sample}_samples_random.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # take random train sample only white\n",
    "# size_sample = 10000\n",
    "# only_white = train_set[train_set[\"Race\"] == \"White\"]\n",
    "# print(len(only_white))\n",
    "# sample_set = train_set.sample(n=size_sample)\n",
    "# sample_set.to_csv(label_dir + f'train_{size_sample}_samples_random_white.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take random fairface train sample\n",
    "# size_sample = 10000\n",
    "# ff_train_set = pd.read_csv(\"fairface/dataset/fairface_label_train.csv\")\n",
    "# ff_train_set = ff_train_set.sample(n=size_sample)\n",
    "# ff_train_set.to_csv(\"fairface/dataset/fairface_label_train_10000_samples.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build diverse and balanced dataset from all augmentation on CelebA dataset (Pick only one augmentation per image) -- Race Score Goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ff_prediction_path = \"fairface/outputs/train_aug_10000_distinct_samples_correct_gender.csv\"\n",
    "# output_csv = \"train_aug_10000_distinct_samples_correct_gender.csv\"\n",
    "# append = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the genders from CelebA and assume they do not change during augmentation (as we specifically told it with the new prompts)\n",
    "# gender_df = pd.DataFrame(np.repeat(df_attr[\"Male\"].values, len(races), axis=0))\n",
    "\n",
    "# # load predictions from FF classification\n",
    "# ff_prediction = pd.read_csv(ff_prediction_path)\n",
    "\n",
    "# # merge the race predictions with the annotated genders and rename the columns\n",
    "# aug_df = ff_prediction.merge(gender_df, left_index=True, right_index=True)\n",
    "# # aug_df =  # male = df[\"Male\"].replace(\"Male\", 1).replace(\"Female\", 0)\n",
    "\n",
    "# aug_df.columns = [\"Image_Name\", \"Race\", \"Male\", \"Race_Score_Goal\", \"Race_Scores\", \"Gender_Scores\", \"Original_Gender\"]\n",
    "\n",
    "# # swap 1 and 0 entries in 'Male' column\n",
    "# aug_df.loc[aug_df['Male'] == 1, 'Male'] = -1\n",
    "# aug_df.loc[aug_df['Male'] == 0, 'Male'] = 1\n",
    "# aug_df.loc[aug_df['Male'] == -1, 'Male'] = 0\n",
    "\n",
    "# # sort the images descending by prediction quality\n",
    "# aug_df = aug_df.sort_values(\"Race_Score_Goal\", ascending=False)\n",
    "\n",
    "# # extract the expected races from the image names\n",
    "# exp_races = [races[int(img_name.split(\"_\")[1])] for img_name in aug_df[\"Image_Name\"].values]\n",
    "\n",
    "# # split predictions by race\n",
    "# grouped_df = aug_df.groupby(exp_races)\n",
    "# race_tables = [grouped_df.get_group(race) for race in races]\n",
    "\n",
    "# # initialize variables for creating the resulting dataset\n",
    "# total_images = int(len(aug_df) / len(races))\n",
    "# remaining_img_nmb = [f\"{(i+1):06}\" for i in range(total_images)]\n",
    "# picked_aug_imgs = pd.DataFrame(columns=aug_df.columns)\n",
    "\n",
    "\n",
    "# # use every original image in exactly one variant\n",
    "# for i in tqdm(range(total_images), desc=\"Build balanced dataset\"):\n",
    "\n",
    "#       # rotate through all races to make them equally represented\n",
    "#       race_index = i % len(races)\n",
    "#       race = races[race_index]\n",
    "\n",
    "#       # take best augmentation for current race of an image which was not already picked\n",
    "#       while True:\n",
    "#             first_row = race_tables[race_index].iloc[0]\n",
    "#             race_tables[race_index] = race_tables[race_index][1:]\n",
    "#             img_number = first_row[\"Image_Name\"].split(\"_\")[0]\n",
    "\n",
    "#             if img_number in remaining_img_nmb:\n",
    "#                   break\n",
    "\n",
    "#       # remove face0 from the end of each image name\n",
    "#       # first_row[\"Image_Name\"] = first_row[\"Image_Name\"].split(\".\")[0][:-6] + \".jpg\"\n",
    "\n",
    "#       # add this augmentation to resulting dataset\n",
    "#       picked_aug_imgs = picked_aug_imgs.append(first_row)\n",
    "\n",
    "#       # do not take any other augmentation of this image\n",
    "#       remaining_img_nmb.remove(img_number)\n",
    "\n",
    "# # save result\n",
    "# picked_aug_imgs.to_csv(label_dir + output_csv, mode=\"a\" if append else \"w\")\n",
    "# print(f\"Saved augmentation dataset at: '{label_dir + output_csv}'\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build diverse and balanced dataset from all augmentation on CelebA dataset (Pick only one augmentation per image) -- Total fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_fid_path = \"DMimageDetection/fidelities_mixed_caps_total_70k.csv\"\n",
    "# total_fid_path = \"DMimageDetection/total_fidelities_10k_ff.csv\"\n",
    "# output_csv = \"train_aug_10k_distinct_samples_harm.csv\"\n",
    "output_csv = \"fidelities_mixed_caps_best_10k_balanced.csv\"\n",
    "# output_csv = \"train_aug_10k_total_fid_ff.csv\"\n",
    "append = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Image_Name    Race  Gender  Race_Score_Goal  \\\n",
      "0            000001_0_black.jpg   Black       1            0.953   \n",
      "1           000001_1_indian.jpg  Latino       1            0.207   \n",
      "2           000001_2_latino.jpg  Latino       1            0.628   \n",
      "3   000001_3_middle eastern.jpg  Latino       1            0.019   \n",
      "4  000001_4_southeast asian.jpg  Latino       1            0.118   \n",
      "\n",
      "                                   Race_Scores Gender_Scores  Fidelity_Score  \\\n",
      "0  [0.953 0.004 0.041 0.    0.    0.    0.002]       [0. 1.]        0.987559   \n",
      "1  [0.004 0.207 0.642 0.041 0.001 0.    0.105]       [0. 1.]        0.979519   \n",
      "2  [0.009 0.003 0.628 0.009 0.001 0.    0.35 ]       [0. 1.]        0.978242   \n",
      "3  [0.005 0.007 0.546 0.019 0.    0.    0.422]       [0. 1.]        0.975398   \n",
      "4  [0.004 0.005 0.717 0.001 0.118 0.042 0.112]       [0. 1.]        0.949471   \n",
      "\n",
      "   Total_Fidelity  Male  \n",
      "0        0.969972     0  \n",
      "1        0.341774     0  \n",
      "2        0.764936     0  \n",
      "3        0.037274     0  \n",
      "4        0.209912     0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Build balanced dataset: 100%|██████████| 10000/10000 [00:50<00:00, 199.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved augmentation dataset csv at: 'CelebA/labels_split/train_aug_10k_total_fid_harm_precise_prompts.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load the genders from CelebA and assume they do not change during augmentation (as we specifically told it with the new prompts)\n",
    "gender_df = pd.DataFrame(np.repeat(df_attr[\"Male\"].values, len(races), axis=0))\n",
    "gender_df.columns = [\"Male\"]\n",
    "# print(gender_df.head())\n",
    "# load predictions from FF classification\n",
    "aug_labels = pd.read_csv(total_fid_path).iloc[: , 1:]\n",
    "# print(fidelities.head())\n",
    "\n",
    "# merge the race predictions with the annotated genders\n",
    "aug_df = aug_labels.merge(gender_df, left_index=True, right_index=True)\n",
    "# print(aug_df.head())\n",
    "\n",
    "\n",
    "# # swap 1 and 0 entries in 'Male' column => Original Gender\n",
    "# aug_df.loc[aug_df[\"Male\"] == 1, \"Male\"] = -1\n",
    "# aug_df.loc[aug_df[\"Male\"] == 0, \"Male\"] = 1\n",
    "# aug_df.loc[aug_df[\"Male\"] == -1, \"Male\"] = 0\n",
    "# aug_df[\"Original_Gender\"] = aug_df[\"Male\"]\n",
    "# aug_df = aug_df.drop(columns=[\"Male\"])\n",
    "\n",
    "# print(aug_df.head())\n",
    "\n",
    "# sort the images descending by prediction quality\n",
    "aug_df = aug_df.sort_values(\"Total_Fidelity\", ascending=False) # Fidelity_Score Race_Score_Goal\"Race_Score_Goal\"\n",
    "# aug_df = aug_df[:10000]\n",
    "# aug_df.to_csv(label_dir + output_csv)\n",
    "\n",
    "\n",
    "# extract the expected races from the image names\n",
    "exp_races = [races[int(img_name.split(\"_\")[1])] for img_name in aug_df[\"Image_Name\"].values]\n",
    "\n",
    "# split predictions by race\n",
    "grouped_df = aug_df.groupby(exp_races)\n",
    "race_tables = [grouped_df.get_group(race) for race in races]\n",
    "\n",
    "# initialize variables for creating the resulting dataset\n",
    "total_images = int(len(aug_df) / len(races))\n",
    "remaining_img_nmb = [f\"{(i+1):06}\" for i in range(total_images)]\n",
    "picked_aug_imgs = pd.DataFrame(columns=aug_df.columns)\n",
    "\n",
    "\n",
    "# use every original image in exactly one variant\n",
    "for i in tqdm(range(total_images), desc=\"Build balanced dataset\"):\n",
    "\n",
    "      # rotate through all races to make them equally represented\n",
    "      race_index = i % len(races)\n",
    "      race = races[race_index]\n",
    "\n",
    "      # take best augmentation for current race of an image which was not already picked\n",
    "      while True:\n",
    "            first_row = race_tables[race_index].iloc[0]\n",
    "            race_tables[race_index] = race_tables[race_index][1:]\n",
    "            img_number = first_row[\"Image_Name\"].split(\"_\")[0]\n",
    "\n",
    "            if img_number in remaining_img_nmb:\n",
    "                  break\n",
    "\n",
    "      # remove face0 from the end of each image name\n",
    "      # first_row[\"Image_Name\"] = first_row[\"Image_Name\"].split(\".\")[0][:-6] + \".jpg\"\n",
    "\n",
    "      # add this augmentation to resulting dataset\n",
    "      picked_aug_imgs = picked_aug_imgs.append(first_row)\n",
    "\n",
    "      # do not take any other augmentation of this image\n",
    "      remaining_img_nmb.remove(img_number)\n",
    "\n",
    "# save result\n",
    "picked_aug_imgs.to_csv(label_dir + output_csv, mode=\"a\" if append else \"w\")\n",
    "print(f\"Saved augmentation dataset csv at: '{label_dir + output_csv}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculate gender accuracy\n",
    "# correct = 0\n",
    "# for index, row in picked_aug_imgs.iterrows():\n",
    "#     exp_race = row[\"Image_Name\"].split(\"_\")[-1].split(\".\")[0]\n",
    "#     ff_race = row[\"Race\"].lower()\n",
    "#     correct += exp_race == ff_race\n",
    "\n",
    "# race_acc = correct/len(picked_aug_imgs)\n",
    "\n",
    "# # calculate mean prediction confidence\n",
    "# mean_confidence = picked_aug_imgs[\"Race_Goal_Score\"].values.mean()\n",
    "\n",
    "# # print results\n",
    "# print(f\"The race classification accuracy is {race_acc:.2%} with a mean prediction confidence of {mean_confidence:.2%}.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare FairFace data for classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Image_Name  Fidelity_Score  Image_Number  Gender  \\\n",
      "0           85109_0_black.jpg        0.999431         85109    Male   \n",
      "1          30458_1_indian.jpg        0.998755         30458  Female   \n",
      "2  85109_3_middle eastern.jpg        0.998699         85109    Male   \n",
      "3  67870_3_middle eastern.jpg        0.998675         67870  Female   \n",
      "4          85109_1_indian.jpg        0.998407         85109    Male   \n",
      "5          68500_1_indian.jpg        0.998260         68500  Female   \n",
      "6          52494_1_indian.jpg        0.998212         52494  Female   \n",
      "7           38509_0_black.jpg        0.998167         38509  Female   \n",
      "8          81978_1_indian.jpg        0.998028         81978  Female   \n",
      "9           85109_6_white.jpg        0.997896         85109    Male   \n",
      "\n",
      "             Race  \n",
      "0           Black  \n",
      "1          Indian  \n",
      "2  Middle Eastern  \n",
      "3  Middle Eastern  \n",
      "4          Indian  \n",
      "5          Indian  \n",
      "6          Indian  \n",
      "7           Black  \n",
      "8          Indian  \n",
      "9           White  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load original fairface labels\n",
    "labels = pd.read_csv('fairface/dataset/labels/fairface_label_train.csv')\n",
    "\n",
    "# load fidelities for augmented images\n",
    "aug_labels = pd.read_csv('DMimageDetection/fidelities_mixed_caps_total_70k.csv')\n",
    "\n",
    "# extract image id from image name\n",
    "def extract_image_number(image_name):\n",
    "    return int(image_name.split('_')[0])\n",
    "\n",
    "# extract gt genders from original ff labels\n",
    "aug_labels[\"Image_Number\"] = aug_labels['Image_Name'].apply(lambda x: extract_image_number(x))\n",
    "aug_labels['Gender'] = aug_labels['Image_Number'].apply(lambda x: labels.loc[x-1]['gender'])\n",
    "\n",
    "# extract race from image name\n",
    "def extract_race(image_name):\n",
    "    race_parts = image_name.split('_')[2].split('.')[0].split()\n",
    "    capitalized_race = ' '.join([part.capitalize() for part in race_parts])\n",
    "    return capitalized_race\n",
    "\n",
    "# extract race from augmented labels\n",
    "aug_labels['Race'] = aug_labels['Image_Name'].apply(extract_race)\n",
    "\n",
    "\n",
    "\n",
    "# show results\n",
    "print(aug_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Build balanced dataset: 100%|██████████| 10000/10000 [00:50<00:00, 199.11it/s]\n"
     ]
    }
   ],
   "source": [
    "races = [\"Black\", \"Indian\", \"Latino\", \"Middle Eastern\", \"Southeast Asian\", \"East Asian\", \"White\"]\n",
    "\n",
    "# sort the images descending by prediction quality\n",
    "aug_labels = aug_labels.sort_values(\"Fidelity_Score\", ascending=False)\n",
    "\n",
    "# split predictions by race\n",
    "grouped_df = aug_labels.groupby(aug_labels[\"Race\"])\n",
    "race_tables = [grouped_df.get_group(race) for race in races]\n",
    "\n",
    "# initialize variables for creating the resulting dataset\n",
    "total_images = int(len(aug_labels) / len(races))\n",
    "remaining_img_nmb = list(set(aug_labels[\"Image_Number\"].values))\n",
    "\n",
    "picked_aug_imgs = pd.DataFrame(columns=aug_labels.columns)\n",
    "\n",
    "# use every original image in exactly one variant\n",
    "for i in tqdm(range(total_images), desc=\"Build balanced dataset\"):\n",
    "     # rotate through all races to make them equally represented\n",
    "      race_index = i % len(races)\n",
    "      race = races[race_index]\n",
    "\n",
    "      # take best augmentation for current race of an image which was not already picked\n",
    "      while True:\n",
    "            first_row = race_tables[race_index].iloc[0]\n",
    "            race_tables[race_index] = race_tables[race_index][1:]\n",
    "            img_number = first_row[\"Image_Number\"]\n",
    "            # print(img_number, img_number.type)\n",
    "\n",
    "            if img_number in remaining_img_nmb:\n",
    "                  break\n",
    "\n",
    "      # add this augmentation to resulting dataset\n",
    "      picked_aug_imgs = picked_aug_imgs.append(first_row)\n",
    "\n",
    "      # do not take any other augmentation of this image\n",
    "      remaining_img_nmb.remove(img_number)\n",
    "      # print(len(remaining_img_nmb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Image_Name  Fidelity_Score Image_Number  Gender  \\\n",
      "0             85109_0_black.jpg        0.999431        85109    Male   \n",
      "1            30458_1_indian.jpg        0.998755        30458  Female   \n",
      "22           68500_2_latino.jpg        0.997100        68500  Female   \n",
      "3    67870_3_middle eastern.jpg        0.998675        67870  Female   \n",
      "48  55748_4_southeast asian.jpg        0.996006        55748  Female   \n",
      "46       28702_5_east asian.jpg        0.996080        28702  Female   \n",
      "10            64553_6_white.jpg        0.997672        64553  Female   \n",
      "7             38509_0_black.jpg        0.998167        38509  Female   \n",
      "6            52494_1_indian.jpg        0.998212        52494  Female   \n",
      "28           12194_2_latino.jpg        0.996740        12194  Female   \n",
      "\n",
      "               Race  \n",
      "0             Black  \n",
      "1            Indian  \n",
      "22           Latino  \n",
      "3    Middle Eastern  \n",
      "48  Southeast Asian  \n",
      "46       East Asian  \n",
      "10            White  \n",
      "7             Black  \n",
      "6            Indian  \n",
      "28           Latino  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# picked_aug_imgs = picked_aug_imgs.replace(\"Male\", 1).replace(\"Female\", 0)\n",
    "print(picked_aug_imgs[:10])\n",
    "picked_aug_imgs.to_csv(\"fairface/dataset/labels/ff_mixed_caps_10k_balanced.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the DataFrame from a file (assuming it's in CSV format)\n",
    "df = pd.read_csv('fairface/dataset/labels/fairface_label_train.csv')\n",
    "\n",
    "# Filter the DataFrame based on the \"Gender\" column\n",
    "filtered_df = df[df['race'] == 'White'].sample(n=10000)\n",
    "\n",
    "# Save the filtered entries as a CSV file\n",
    "filtered_df.to_csv('fairface/dataset/labels/ff_train_only_white_10k.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"fairface/dataset/labels/fairface_label_train.csv\")\n",
    "dataset = dataset.sample(n=10000)\n",
    "dataset.to_csv(\"fairface/dataset/labels/fairface_label_train_random_10k.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1 saved as fairface/dataset/labels/permutations/train_1.csv\n",
      "Table 2 saved as fairface/dataset/labels/permutations/train_2.csv\n",
      "Table 3 saved as fairface/dataset/labels/permutations/train_3.csv\n",
      "Table 4 saved as fairface/dataset/labels/permutations/train_4.csv\n",
      "Table 5 saved as fairface/dataset/labels/permutations/train_5.csv\n",
      "Table 6 saved as fairface/dataset/labels/permutations/train_6.csv\n",
      "Table 7 saved as fairface/dataset/labels/permutations/train_7.csv\n",
      "Table 8 saved as fairface/dataset/labels/permutations/train_8.csv\n",
      "Table 9 saved as fairface/dataset/labels/permutations/train_9.csv\n",
      "Table 10 saved as fairface/dataset/labels/permutations/train_10.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create the \"permutations\" directory if it doesn't exist\n",
    "directory = \"fairface/dataset/labels/permutations\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Load the original CSV data\n",
    "dataset = pd.read_csv(\"fairface/dataset/labels/fairface_label_train.csv\")\n",
    "\n",
    "# Repeat the process 10 times\n",
    "for i in range(10):\n",
    "    # Take a random sample\n",
    "    selected_data = dataset.sample(n=10000)\n",
    "\n",
    "    # Save the shuffled and selected data as a new CSV file\n",
    "    filename = f\"fairface/dataset/labels/permutations/train_{i+1}.csv\"\n",
    "    selected_data.to_csv(filename, index=False)\n",
    "    print(f\"Table {i+1} saved as {filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
