{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_path = \"datasets/celeba/list_attr_celeba.txt\"\n",
    "partitions_path = 'datasets/celeba/list_eval_partition.txt'\n",
    "race_path = \"CelebA/races/races_ff.csv\"\n",
    "label_dir = \"CelebA/labels_split/\"\n",
    "img_celeb_dir = \"CelebA/cropped/\"\n",
    "# img_created_dir = \"CelebA/augmented/raw_input_auto_gender_cropped\"\n",
    "races = [\"Black\", \"Indian\", \"Latino\", \"Middle Eastern\", \"Southeast Asian\", \"East Asian\", \"White\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all labels\n",
    "\n",
    "# all_labels = pd.read_csv(attributes_path, sep=\"\\s+\", skiprows=1).rename_axis(\"Image_Name\").reset_index().replace(-1, 0)\n",
    "# print(all_labels.head())\n",
    "\n",
    "# # create training and validation labels out off all_labels\n",
    "# y_train = all_labels[:size_train]\n",
    "# # y_train.columns = all_labels.columns\n",
    "# print(y_train.shape)\n",
    "\n",
    "# y_train = pd.DataFrame(columns=all_labels.columns)\n",
    "# for i, row in tqdm(all_labels[:size_train].iterrows(), total=size_train):\n",
    "#     img_number = f\"{(i+1):06}\"\n",
    "#     for j, race in enumerate(races):\n",
    "#         new_image_name = f\"{img_number}_{j}_{race.lower()}.jpg\"\n",
    "#         row[0] = new_image_name\n",
    "#         y_train = y_train.append(row)\n",
    "#     # if i == 0:\n",
    "#     #     break\n",
    "# print(y_train.shape)\n",
    "# y_train.to_csv(label_dir + \"created_train2.csv\")\n",
    "\n",
    "# y_val = all_labels[-1000:]\n",
    "# y_val.to_csv(label_dir + \"val.csv\")\n",
    "\n",
    "# y_test = all_labels[-2000:-1000]\n",
    "# y_test.to_csv(label_dir + \"test.csv\")\n",
    "\n",
    "# print(\"y_train shape:\", y_train.shape)\n",
    "# print(\"y_val shape:\", y_val.shape)\n",
    "# print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162770 19867 19962\n"
     ]
    }
   ],
   "source": [
    "df_attr = pd.read_csv(attributes_path, sep=\"\\s+\", skiprows=1)\n",
    "df_attr = df_attr.replace(-1, 0)\n",
    "# df_attr.head()\n",
    "\n",
    "df_part = pd.read_csv(partitions_path, sep=\"\\s+\", skiprows=0, header=None)\n",
    "df_part.columns = ['Image_Name', 'Partition']\n",
    "df_part = df_part.set_index('Image_Name')\n",
    "# print(df_part.head())\n",
    "\n",
    "df_attr_part = df_attr.merge(df_part, left_index=True, right_index=True)\n",
    "df_attr_part.index = df_attr_part.index.rename('Image_Name')\n",
    "# print(df_attr_part.columns)\n",
    "\n",
    "df_race = pd.read_csv(race_path, usecols=[\"face_name_align\", \"race\"])\n",
    "df_race.columns = ['Image_Name', 'Race']\n",
    "df_race = df_race.set_index('Image_Name')\n",
    "# print(df_race.head())\n",
    "\n",
    "df_total = df_attr_part.merge(df_race, left_index=True, right_index=True)\n",
    "# df_total.head()\n",
    "\n",
    "\n",
    "# realize standard celeba split\n",
    "train_set = df_total.loc[df_total['Partition'] == 0].drop(\"Partition\", axis=1)\n",
    "val_set = df_total.loc[df_total['Partition'] == 1].drop(\"Partition\", axis=1)\n",
    "test_set = df_total.loc[df_total['Partition'] == 2].drop(\"Partition\", axis=1)\n",
    "\n",
    "print(len(train_set), len(val_set), len(test_set))\n",
    "# train_set.to_csv(label_dir + 'train_total.csv')\n",
    "# val_set.to_csv(label_dir + 'val_total.csv')\n",
    "# test_set.to_csv(label_dir + 'test_total.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training labels for augmented data (expecting leading sample)\n",
    "# size_augmented_data = 1000\n",
    "# df_attr_prepared = df_attr[:size_augmented_data].rename_axis(\"Image_Name\").reset_index()\n",
    "# train_set_aug = pd.DataFrame(columns=df_attr_prepared.columns)\n",
    "# # print(train_set_aug.head())\n",
    "# for i, row in tqdm(df_attr_prepared.iterrows(), total=size_augmented_data):\n",
    "#     img_number = f\"{(i+1):06}\"\n",
    "#     for j, race in enumerate(races):\n",
    "#         new_image_name = f\"{img_number}_{j}_{race.lower()}.jpg\"\n",
    "#         row[0] = new_image_name\n",
    "#         row[\"Race\"] = race\n",
    "#         train_set_aug = train_set_aug.append(row)\n",
    "# train_set_aug = train_set_aug.set_index(\"Image_Name\")\n",
    "# print(train_set_aug.shape)\n",
    "# train_set_aug.to_csv(label_dir + f\"train_aug_{size_augmented_data}_samples.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split validation set by race\n",
    "# grouped_df = val_set.groupby(val_set.Race)\n",
    "# val_races = []\n",
    "# print(f\"Validation set ({len(val_set)} images):\")\n",
    "# for race in races:\n",
    "#       val_race = grouped_df.get_group(race)\n",
    "#       print(f\"{race}: {len(val_race)}\", end=\"\\t\")\n",
    "#       val_race.to_csv(label_dir+\"val/\"+race+\".csv\")\n",
    "# print(\"\\n\")\n",
    "\n",
    "# # split test set by race\n",
    "# grouped_df = test_set.groupby(test_set.Race)\n",
    "# test_races = []\n",
    "# print(f\"Test set ({len(val_set)} images):\")\n",
    "# for race in races:\n",
    "#       test_race = grouped_df.get_group(race)\n",
    "#       print(f\"{race}: {len(test_race)}\", end=\"\\t\")\n",
    "#       test_race.to_csv(label_dir+\"test/\"+race+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take random train sample\n",
    "# size_sample = 500\n",
    "# rand_perm = np.random.permutation(len(train_set))\n",
    "# sample_set = train_set.iloc[rand_perm[:size_sample]]\n",
    "# sample_set.to_csv(label_dir + f'train_{size_sample}_samples_shuffled.csv')\n",
    "# print(sample_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # take leading train sample\n",
    "# size_sample = 1000\n",
    "# only_white = train_set[train_set[\"Race\"] == \"White\"]\n",
    "# sample_set = only_white[:size_sample]\n",
    "# sample_set.to_csv(label_dir + f'train_{size_sample}_samples_only_white.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take random train sample\n",
    "# size_sample = 10000\n",
    "# train_sample = train_set.sample(n=size_sample)\n",
    "# train_sample.to_csv(label_dir + f'train_{size_sample}_samples_random.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # take random train sample only white\n",
    "# size_sample = 10000\n",
    "# only_white = train_set[train_set[\"Race\"] == \"White\"]\n",
    "# print(len(only_white))\n",
    "# sample_set = train_set.sample(n=size_sample)\n",
    "# sample_set.to_csv(label_dir + f'train_{size_sample}_samples_random_white.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take random fairface train sample\n",
    "# size_sample = 10000\n",
    "# ff_train_set = pd.read_csv(\"fairface/dataset/fairface_label_train.csv\")\n",
    "# ff_train_set = ff_train_set.sample(n=size_sample)\n",
    "# ff_train_set.to_csv(\"fairface/dataset/fairface_label_train_10000_samples.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build diverse and balanced dataset from all augmentation on CelebA dataset (Pick only one augmentation per image) -- Race Score Goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ff_prediction_path = \"fairface/outputs/train_aug_10000_distinct_samples_correct_gender.csv\"\n",
    "# output_csv = \"train_aug_10000_distinct_samples_correct_gender.csv\"\n",
    "# append = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the genders from CelebA and assume they do not change during augmentation (as we specifically told it with the new prompts)\n",
    "# gender_df = pd.DataFrame(np.repeat(df_attr[\"Male\"].values, len(races), axis=0))\n",
    "\n",
    "# # load predictions from FF classification\n",
    "# ff_prediction = pd.read_csv(ff_prediction_path)\n",
    "\n",
    "# # merge the race predictions with the annotated genders and rename the columns\n",
    "# aug_df = ff_prediction.merge(gender_df, left_index=True, right_index=True)\n",
    "# # aug_df =  # male = df[\"Male\"].replace(\"Male\", 1).replace(\"Female\", 0)\n",
    "\n",
    "# aug_df.columns = [\"Image_Name\", \"Race\", \"Male\", \"Race_Score_Goal\", \"Race_Scores\", \"Gender_Scores\", \"Original_Gender\"]\n",
    "\n",
    "# # swap 1 and 0 entries in 'Male' column\n",
    "# aug_df.loc[aug_df['Male'] == 1, 'Male'] = -1\n",
    "# aug_df.loc[aug_df['Male'] == 0, 'Male'] = 1\n",
    "# aug_df.loc[aug_df['Male'] == -1, 'Male'] = 0\n",
    "\n",
    "# # sort the images descending by prediction quality\n",
    "# aug_df = aug_df.sort_values(\"Race_Score_Goal\", ascending=False)\n",
    "\n",
    "# # extract the expected races from the image names\n",
    "# exp_races = [races[int(img_name.split(\"_\")[1])] for img_name in aug_df[\"Image_Name\"].values]\n",
    "\n",
    "# # split predictions by race\n",
    "# grouped_df = aug_df.groupby(exp_races)\n",
    "# race_tables = [grouped_df.get_group(race) for race in races]\n",
    "\n",
    "# # initialize variables for creating the resulting dataset\n",
    "# total_images = int(len(aug_df) / len(races))\n",
    "# remaining_img_nmb = [f\"{(i+1):06}\" for i in range(total_images)]\n",
    "# picked_aug_imgs = pd.DataFrame(columns=aug_df.columns)\n",
    "\n",
    "\n",
    "# # use every original image in exactly one variant\n",
    "# for i in tqdm(range(total_images), desc=\"Build balanced dataset\"):\n",
    "\n",
    "#       # rotate through all races to make them equally represented\n",
    "#       race_index = i % len(races)\n",
    "#       race = races[race_index]\n",
    "\n",
    "#       # take best augmentation for current race of an image which was not already picked\n",
    "#       while True:\n",
    "#             first_row = race_tables[race_index].iloc[0]\n",
    "#             race_tables[race_index] = race_tables[race_index][1:]\n",
    "#             img_number = first_row[\"Image_Name\"].split(\"_\")[0]\n",
    "\n",
    "#             if img_number in remaining_img_nmb:\n",
    "#                   break\n",
    "\n",
    "#       # remove face0 from the end of each image name\n",
    "#       # first_row[\"Image_Name\"] = first_row[\"Image_Name\"].split(\".\")[0][:-6] + \".jpg\"\n",
    "\n",
    "#       # add this augmentation to resulting dataset\n",
    "#       picked_aug_imgs = picked_aug_imgs.append(first_row)\n",
    "\n",
    "#       # do not take any other augmentation of this image\n",
    "#       remaining_img_nmb.remove(img_number)\n",
    "\n",
    "# # save result\n",
    "# picked_aug_imgs.to_csv(label_dir + output_csv, mode=\"a\" if append else \"w\")\n",
    "# print(f\"Saved augmentation dataset at: '{label_dir + output_csv}'\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build diverse and balanced dataset from all augmentation on CelebA dataset (Pick only one augmentation per image) -- Total fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_fid_path = \"/workspaces/PromptToPrompt/DMimageDetection/total_fidelities_10k_harm_precise_prompts.csv\"\n",
    "# output_csv = \"train_aug_10k_distinct_samples_harm.csv\"\n",
    "output_csv = \"train_aug_10k_total_fid_harm_precise_prompts.csv\"\n",
    "append = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Image_Name    Race  Gender  Race_Score_Goal  \\\n",
      "0            000001_0_black.jpg   Black       1            0.953   \n",
      "1           000001_1_indian.jpg  Latino       1            0.207   \n",
      "2           000001_2_latino.jpg  Latino       1            0.628   \n",
      "3   000001_3_middle eastern.jpg  Latino       1            0.019   \n",
      "4  000001_4_southeast asian.jpg  Latino       1            0.118   \n",
      "\n",
      "                                   Race_Scores Gender_Scores  Fidelity_Score  \\\n",
      "0  [0.953 0.004 0.041 0.    0.    0.    0.002]       [0. 1.]        0.987559   \n",
      "1  [0.004 0.207 0.642 0.041 0.001 0.    0.105]       [0. 1.]        0.979519   \n",
      "2  [0.009 0.003 0.628 0.009 0.001 0.    0.35 ]       [0. 1.]        0.978242   \n",
      "3  [0.005 0.007 0.546 0.019 0.    0.    0.422]       [0. 1.]        0.975398   \n",
      "4  [0.004 0.005 0.717 0.001 0.118 0.042 0.112]       [0. 1.]        0.949471   \n",
      "\n",
      "   Total_Fidelity  Male  \n",
      "0        0.969972     0  \n",
      "1        0.341774     0  \n",
      "2        0.764936     0  \n",
      "3        0.037274     0  \n",
      "4        0.209912     0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Build balanced dataset: 100%|██████████| 10000/10000 [00:50<00:00, 199.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved augmentation dataset csv at: 'CelebA/labels_split/train_aug_10k_total_fid_harm_precise_prompts.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load the genders from CelebA and assume they do not change during augmentation (as we specifically told it with the new prompts)\n",
    "gender_df = pd.DataFrame(np.repeat(df_attr[\"Male\"].values, len(races), axis=0))\n",
    "gender_df.columns = [\"Male\"]\n",
    "# print(gender_df.head())\n",
    "# load predictions from FF classification\n",
    "fidelities = pd.read_csv(total_fid_path).iloc[: , 1:]\n",
    "# print(fidelities.head())\n",
    "\n",
    "# merge the race predictions with the annotated genders\n",
    "aug_df = fidelities.merge(gender_df, left_index=True, right_index=True)\n",
    "# print(aug_df.head())\n",
    "\n",
    "\n",
    "# # swap 1 and 0 entries in 'Male' column => Original Gender\n",
    "# aug_df.loc[aug_df[\"Male\"] == 1, \"Male\"] = -1\n",
    "# aug_df.loc[aug_df[\"Male\"] == 0, \"Male\"] = 1\n",
    "# aug_df.loc[aug_df[\"Male\"] == -1, \"Male\"] = 0\n",
    "# aug_df[\"Original_Gender\"] = aug_df[\"Male\"]\n",
    "# aug_df = aug_df.drop(columns=[\"Male\"])\n",
    "\n",
    "# print(aug_df.head())\n",
    "\n",
    "# sort the images descending by prediction quality\n",
    "aug_df = aug_df.sort_values(\"Total_Fidelity\", ascending=False) # Fidelity_Score Race_Score_Goal\"Race_Score_Goal\"\n",
    "# aug_df = aug_df[:10000]\n",
    "# aug_df.to_csv(label_dir + output_csv)\n",
    "\n",
    "\n",
    "# extract the expected races from the image names\n",
    "exp_races = [races[int(img_name.split(\"_\")[1])] for img_name in aug_df[\"Image_Name\"].values]\n",
    "\n",
    "# split predictions by race\n",
    "grouped_df = aug_df.groupby(exp_races)\n",
    "race_tables = [grouped_df.get_group(race) for race in races]\n",
    "\n",
    "# initialize variables for creating the resulting dataset\n",
    "total_images = int(len(aug_df) / len(races))\n",
    "remaining_img_nmb = [f\"{(i+1):06}\" for i in range(total_images)]\n",
    "picked_aug_imgs = pd.DataFrame(columns=aug_df.columns)\n",
    "\n",
    "\n",
    "# use every original image in exactly one variant\n",
    "for i in tqdm(range(total_images), desc=\"Build balanced dataset\"):\n",
    "\n",
    "      # rotate through all races to make them equally represented\n",
    "      race_index = i % len(races)\n",
    "      race = races[race_index]\n",
    "\n",
    "      # take best augmentation for current race of an image which was not already picked\n",
    "      while True:\n",
    "            first_row = race_tables[race_index].iloc[0]\n",
    "            race_tables[race_index] = race_tables[race_index][1:]\n",
    "            img_number = first_row[\"Image_Name\"].split(\"_\")[0]\n",
    "\n",
    "            if img_number in remaining_img_nmb:\n",
    "                  break\n",
    "\n",
    "      # remove face0 from the end of each image name\n",
    "      # first_row[\"Image_Name\"] = first_row[\"Image_Name\"].split(\".\")[0][:-6] + \".jpg\"\n",
    "\n",
    "      # add this augmentation to resulting dataset\n",
    "      picked_aug_imgs = picked_aug_imgs.append(first_row)\n",
    "\n",
    "      # do not take any other augmentation of this image\n",
    "      remaining_img_nmb.remove(img_number)\n",
    "\n",
    "# save result\n",
    "picked_aug_imgs.to_csv(label_dir + output_csv, mode=\"a\" if append else \"w\")\n",
    "print(f\"Saved augmentation dataset csv at: '{label_dir + output_csv}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculate gender accuracy\n",
    "# correct = 0\n",
    "# for index, row in picked_aug_imgs.iterrows():\n",
    "#     exp_race = row[\"Image_Name\"].split(\"_\")[-1].split(\".\")[0]\n",
    "#     ff_race = row[\"Race\"].lower()\n",
    "#     correct += exp_race == ff_race\n",
    "\n",
    "# race_acc = correct/len(picked_aug_imgs)\n",
    "\n",
    "# # calculate mean prediction confidence\n",
    "# mean_confidence = picked_aug_imgs[\"Race_Goal_Score\"].values.mean()\n",
    "\n",
    "# # print results\n",
    "# print(f\"The race classification accuracy is {race_acc:.2%} with a mean prediction confidence of {mean_confidence:.2%}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
