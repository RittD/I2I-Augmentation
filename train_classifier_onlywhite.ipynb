{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe1916f3490>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# os.system(\"pip install pandas\")\n",
    "# os.system(\"pip install torchvision\")\n",
    "os.system(\"CUDA_LAUNCH_BLOCKING=1\")\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose experiment\n",
    "size_train = 10000 #86744\n",
    "experiments = [\"FairFace\", \"CelebA\", \"CelebA only white\", \"CelebA augmented\"]\n",
    "exp = 2\n",
    "\n",
    "# set celeb paths\n",
    "celeb_attr_path = \"datasets/celeba/list_attr_celeba.txt\"\n",
    "celeb_partitions_path = 'datasets/celeba/list_eval_partition.txt'\n",
    "celeb_race_path = \"CelebA/races/races_ff.csv\"\n",
    "celeb_label_dir = \"CelebA/labels_split/\"\n",
    "celeb_img_dir = \"CelebA/cropped/\"\n",
    "celeb_img_aug_dir = \"CelebA/augmented/\"\n",
    "celeb_train_csv = f\"train_{size_train}_samples_random.csv\" # \"train_total.csv\"\n",
    "celeb_train_only_white_csv = f\"train_{size_train}_samples_random_white.csv\"\n",
    "celeb_train_aug_csv = f\"train_aug_{size_train}_samples.csv\"\n",
    "celeb_val_csv = \"val_total.csv\"\n",
    "celeb_test_csv = \"test_total.csv\"\n",
    "\n",
    "\n",
    "# set fairface paths\n",
    "ff_img_dir = \"fairface/dataset/fairface-img-margin125-trainval\"\n",
    "ff_label_dir = \"fairface/dataset/\"\n",
    "ff_train_csv = \"fairface_label_train.csv\"\n",
    "ff_val_csv = \"fairface_label_val.csv\"\n",
    "\n",
    "\n",
    "# set hyperparameters\n",
    "learning_rates = [5e-5, 5e-5, 5e-5, 2e-5]\n",
    "lr = learning_rates[exp]\n",
    "num_epochs = 10\n",
    "\n",
    "# Architecture\n",
    "feat_size = (256, 256)\n",
    "bs_train = 128\n",
    "bs_val = 128\n",
    "bs_test = 128\n",
    "device = 'cuda:4'\n",
    "\n",
    "\n",
    "races = [\"Black\", \"Indian\", \"Latino\", \"Middle Eastern\", \"Southeast Asian\", \"East Asian\", \"White\"]\n",
    "ignored_attributes = [\"Black_Hair\", \"Blond_Hair\", \"Brown_Hair\", \"Pale_Skin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define datasets\n",
    "class CelebaDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading CelebA face images\"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, img_dir, transform=None, ignored_attributes=[]):\n",
    "    \n",
    "        df = pd.read_csv(csv_path, index_col=None)\n",
    "        # print(df.head())\n",
    "        self.img_dir = img_dir\n",
    "        self.csv_path = csv_path\n",
    "        self.img_names = df[\"Image_Name\"].values\n",
    "        self.races = df[\"Race\"].values\n",
    "        drop_cols = [\"Image_Name\", \"Race\"] + ignored_attributes\n",
    "        self.y = np.expand_dims(np.array(df[\"Male\"].values), axis=1) #df.drop(drop_cols, axis=1).values #\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(os.path.join(self.img_dir,\n",
    "                                      self.img_names[index]))\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = self.y[index]\n",
    "        gt_race = self.races[index]\n",
    "        return img, label, gt_race\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "    \n",
    "\n",
    "\n",
    "class FairFaceDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading FairFace images\"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, img_dir, transform=None):\n",
    "    \n",
    "        df = pd.read_csv(csv_path, index_col=None)\n",
    "        # print(df.head())\n",
    "        self.img_dir = img_dir\n",
    "        self.csv_path = csv_path\n",
    "        self.img_names = df[\"file\"].values\n",
    "        self.races = df[\"race\"].replace(\"Latino_Hispanic\", \"Latino\").values\n",
    "        gender = df[\"gender\"].replace(\"Male\", 1).replace(\"Female\", 0)\n",
    "        self.y = np.expand_dims(np.array(gender.values), axis=1)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(os.path.join(self.img_dir,\n",
    "                                      self.img_names[index]))\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = self.y[index]\n",
    "        gt_race = self.races[index]\n",
    "        return img, label, gt_race\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets based on current experiment\n",
    "num_workers = 6\n",
    "custom_transform = transforms.Compose([transforms.Resize(feat_size),\n",
    "                                       transforms.ToTensor()])\n",
    "\n",
    "# training dataset\n",
    "if experiments[exp].startswith(\"CelebA\"):\n",
    "\n",
    "    if experiments[exp].endswith(\"augmented\"):\n",
    "        train_csv = celeb_train_aug_csv\n",
    "        train_img_dir = celeb_img_aug_dir\n",
    "    \n",
    "    else:\n",
    "        if \"only white\" in experiments[exp]:\n",
    "            train_csv = celeb_train_only_white_csv\n",
    "        else:\n",
    "            train_csv = celeb_train_csv\n",
    "        train_img_dir = celeb_img_dir\n",
    "\n",
    "    train_dataset = CelebaDataset(csv_path=celeb_label_dir + train_csv,\n",
    "                                img_dir=train_img_dir,\n",
    "                                transform=custom_transform,\n",
    "                                ignored_attributes=ignored_attributes)\n",
    "\n",
    "if experiments[exp].startswith(\"FairFace\"):\n",
    "    train_dataset = FairFaceDataset(csv_path=ff_label_dir + ff_train_csv,\n",
    "                                    img_dir=ff_img_dir,\n",
    "                                    transform=custom_transform)\n",
    "\n",
    "\n",
    "# validation dataset\n",
    "val_dataset = FairFaceDataset(csv_path=ff_label_dir + ff_val_csv,\n",
    "                                img_dir=ff_img_dir,\n",
    "                                transform=custom_transform)\n",
    "\n",
    "# val_dataset = CelebaDataset(csv_path=celeb_label_dir + celeb_val_csv,\n",
    "#                             img_dir=celeb_img_dir,\n",
    "#                             transform=custom_transform,\n",
    "#                             ignored_attributes=ignored_attributes)\n",
    "\n",
    "\n",
    "# test datasets\n",
    "test_dataset_celeb = CelebaDataset(csv_path=celeb_label_dir + celeb_test_csv,\n",
    "                            img_dir=celeb_img_dir,\n",
    "                            transform=custom_transform,\n",
    "                            ignored_attributes=ignored_attributes)\n",
    "\n",
    "test_dataset_ff = val_dataset\n",
    "\n",
    "\n",
    "# create dataloaders on these datasets\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=bs_train,\n",
    "                          shuffle=True,\n",
    "                          num_workers=num_workers)\n",
    "\n",
    "val_loader = DataLoader(dataset=val_dataset,\n",
    "                          batch_size=bs_val,\n",
    "                          shuffle=False,\n",
    "                          num_workers=num_workers)\n",
    "\n",
    "test_loader_celeb = DataLoader(dataset=test_dataset_celeb,\n",
    "                          batch_size=bs_test,\n",
    "                          shuffle=False,\n",
    "                          num_workers=num_workers)\n",
    "\n",
    "test_loader_ff = DataLoader(dataset=test_dataset_ff,\n",
    "                          batch_size=bs_test,\n",
    "                          shuffle=False,\n",
    "                          num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model, define loss and create optimizer\n",
    "model = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "model.to(device)\n",
    "num_attr_predicted = train_dataset.y.shape[1]\n",
    "fc_layer = nn.Linear(1000, num_attr_predicted, device=device)\n",
    "sigmoid = nn.Sigmoid()\n",
    "bin_ce = nn.BCELoss()\n",
    "params = list(model.parameters()) + list(fc_layer.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluation procedure\n",
    "def evaluate_metrics(model, data_loader, device, show_tqdm=False):\n",
    "\n",
    "    correct_predictions = np.zeros(len(races))\n",
    "    true_pos = np.zeros(len(races))\n",
    "    true_neg = np.zeros(len(races))\n",
    "    positive_preds = np.zeros(len(races))\n",
    "    positive_targets = np.zeros(len(races))\n",
    "    num_examples = np.zeros(len(races))\n",
    "    total_examples = len(data_loader.dataset) \n",
    "\n",
    "    # total_it = int(np.ceil(total_examples / data_loader.batch_size))\n",
    "    for _, (features, targets, gt_races) in tqdm(enumerate(data_loader), total=len(data_loader), desc=\"Evaluating\", disable=not show_tqdm):\n",
    "\n",
    "        features = features.to(device)\n",
    "        probas = sigmoid(fc_layer(model(features)))\n",
    "        prediction = (probas >= 0.5).cpu().numpy()\n",
    "        targets = targets.numpy()\n",
    "\n",
    "        # prepape annotated races for metric split afterwards\n",
    "        gt_races = np.array([races.index(race) for race in gt_races])\n",
    "        gt_races = np.expand_dims(gt_races, axis=1)\n",
    "        gt_races = np.broadcast_to(gt_races, prediction.shape)\n",
    "\n",
    "        # collect the necessary data split by annotated race\n",
    "        for j in range(len(races)):\n",
    "            correct_preds = (gt_races == j) & (prediction == targets)\n",
    "            true_pos[j] += (correct_preds & (prediction == 1)).sum()\n",
    "            true_neg[j] += (correct_preds & (prediction == 0)).sum()\n",
    "            correct_predictions[j] += correct_preds.sum()\n",
    "            positive_targets[j] += ((gt_races == j) & (targets == 1)).sum()\n",
    "            positive_preds[j] += np.where(gt_races == j, prediction, 0).sum()\n",
    "            num_examples[j] += (gt_races == j).sum()\n",
    "\n",
    "    # calculate and return metrics    \n",
    "    zero = 1e-10\n",
    "    print(\"Race distribution:\", num_examples/targets.shape[1], \"Total:\", total_examples)\n",
    "\n",
    "    total_accuracy = correct_predictions.sum() / num_examples.sum()\n",
    "    accuracies = correct_predictions / (num_examples + zero)\n",
    "    accs_out = [f\"{a:.2%}\" for a in accuracies]\n",
    "    max_acc_disparity = np.log(max(accuracies)/min(accuracies))\n",
    "\n",
    "    total_precision = true_pos.sum() / (positive_preds.sum() + zero)\n",
    "    precisions = [f\"{p:.2%}\" for p in true_pos / (positive_preds + zero)]\n",
    "\n",
    "    total_recall = true_pos.sum() / (positive_targets.sum() + zero)\n",
    "    recalls = [f\"{r:.2%}\" for r in true_pos / (positive_targets + zero)]\n",
    "    return total_accuracy, accs_out, max_acc_disparity, total_precision, precisions, total_recall, recalls\n",
    "\n",
    "\n",
    "def get_elapsed_time(start_time):\n",
    "    elapsed = int(time.time() - start_time)\n",
    "    m, s = divmod(elapsed, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return f\"{h}:{m:02d}:{s:02d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating experiment 'CelebA only white' with a lr of 5e-05 and 86744 samples on device cuda:4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 01/10: 100%|██████████| 678/678 [03:53<00:00,  2.90it/s, loss=0.0172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race distribution: [1556. 1516. 1623. 1209. 1415. 1550. 2085.] Total: 10954\n",
      "Evaluation epoch 01/10:\n",
      "Total accuracy: 82.70%\t| Accuracies:\t['75.19%', '82.92%', '87.06%', '88.09%', '78.80%', '78.39%', '87.48%'] | Max disparity: 0.1583\n",
      "Total precision: 88.23%\t| Precisions:\t['79.80%', '85.19%', '88.71%', '93.39%', '90.96%', '88.37%', '90.20%']\n",
      "Total recall: 77.64%\t| Recalls:\t['69.21%', '79.42%', '84.24%', '88.56%', '65.71%', '65.51%', '86.10%']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 02/10: 100%|██████████| 678/678 [03:54<00:00,  2.90it/s, loss=0.0142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race distribution: [1556. 1516. 1623. 1209. 1415. 1550. 2085.] Total: 10954\n",
      "Evaluation epoch 02/10:\n",
      "Total accuracy: 80.12%\t| Accuracies:\t['72.04%', '77.44%', '82.75%', '86.19%', '79.93%', '79.81%', '82.88%'] | Max disparity: 0.1792\n",
      "Total precision: 76.36%\t| Precisions:\t['68.02%', '70.61%', '76.36%', '86.05%', '77.33%', '77.75%', '79.00%']\n",
      "Total recall: 90.38%\t| Recalls:\t['85.98%', '93.49%', '93.69%', '94.83%', '86.80%', '83.66%', '92.87%']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 03/10: 100%|██████████| 678/678 [03:51<00:00,  2.93it/s, loss=0.0019]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race distribution: [1556. 1516. 1623. 1209. 1415. 1550. 2085.] Total: 10954\n",
      "Evaluation epoch 03/10:\n",
      "Total accuracy: 81.42%\t| Accuracies:\t['73.65%', '79.35%', '84.66%', '87.26%', '80.28%', '80.39%', '84.36%'] | Max disparity: 0.1696\n",
      "Total precision: 78.45%\t| Precisions:\t['68.72%', '73.45%', '79.44%', '88.54%', '80.00%', '79.09%', '81.24%']\n",
      "Total recall: 89.43%\t| Recalls:\t['89.36%', '91.50%', '92.56%', '93.11%', '82.72%', '82.75%', '92.25%']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 04/10: 100%|██████████| 678/678 [04:01<00:00,  2.81it/s, loss=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race distribution: [1556. 1516. 1623. 1209. 1415. 1550. 2085.] Total: 10954\n",
      "Evaluation epoch 04/10:\n",
      "Total accuracy: 82.96%\t| Accuracies:\t['76.86%', '82.72%', '85.46%', '86.60%', '80.28%', '80.90%', '86.95%'] | Max disparity: 0.1233\n",
      "Total precision: 85.04%\t| Precisions:\t['79.15%', '81.51%', '84.51%', '90.74%', '86.66%', '84.21%', '87.35%']\n",
      "Total recall: 82.23%\t| Recalls:\t['74.59%', '84.33%', '86.00%', '89.18%', '73.33%', '76.19%', '88.59%']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 05/10: 100%|██████████| 678/678 [04:05<00:00,  2.76it/s, loss=0.0028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race distribution: [1556. 1516. 1623. 1209. 1415. 1550. 2085.] Total: 10954\n",
      "Evaluation epoch 05/10:\n",
      "Total accuracy: 82.89%\t| Accuracies:\t['75.13%', '83.31%', '85.09%', '87.10%', '80.85%', '81.87%', '86.38%'] | Max disparity: 0.1478\n",
      "Total precision: 84.39%\t| Precisions:\t['75.75%', '81.17%', '83.31%', '91.74%', '85.58%', '84.54%', '87.75%']\n",
      "Total recall: 82.99%\t| Recalls:\t['75.84%', '86.45%', '86.89%', '88.81%', '75.92%', '78.12%', '86.81%']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 06/10: 100%|██████████| 678/678 [04:10<00:00,  2.71it/s, loss=0.0055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race distribution: [1556. 1516. 1623. 1209. 1415. 1550. 2085.] Total: 10954\n",
      "Evaluation epoch 06/10:\n",
      "Total accuracy: 81.03%\t| Accuracies:\t['73.71%', '78.03%', '82.81%', '86.02%', '80.71%', '81.35%', '84.36%'] | Max disparity: 0.1544\n",
      "Total precision: 78.31%\t| Precisions:\t['70.10%', '71.92%', '76.94%', '87.79%', '80.64%', '80.20%', '81.49%']\n",
      "Total recall: 88.69%\t| Recalls:\t['85.11%', '91.50%', '92.56%', '92.00%', '82.72%', '83.40%', '91.80%']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 07/10: 100%|██████████| 678/678 [04:18<00:00,  2.63it/s, loss=0.0003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race distribution: [1556. 1516. 1623. 1209. 1415. 1550. 2085.] Total: 10954\n",
      "Evaluation epoch 07/10:\n",
      "Total accuracy: 83.21%\t| Accuracies:\t['77.06%', '81.99%', '85.52%', '87.68%', '81.48%', '81.68%', '86.62%'] | Max disparity: 0.1291\n",
      "Total precision: 83.11%\t| Precisions:\t['74.07%', '78.57%', '82.67%', '91.19%', '84.63%', '85.06%', '86.18%']\n",
      "Total recall: 85.65%\t| Recalls:\t['85.11%', '87.65%', '89.03%', '90.41%', '78.64%', '76.96%', '89.48%']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 08/10: 100%|██████████| 678/678 [04:11<00:00,  2.69it/s, loss=0.0201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race distribution: [1556. 1516. 1623. 1209. 1415. 1550. 2085.] Total: 10954\n",
      "Evaluation epoch 08/10:\n",
      "Total accuracy: 84.98%\t| Accuracies:\t['77.76%', '84.83%', '87.62%', '89.08%', '83.32%', '83.81%', '88.06%'] | Max disparity: 0.1359\n",
      "Total precision: 86.32%\t| Precisions:\t['77.59%', '82.40%', '86.54%', '93.16%', '88.68%', '87.36%', '88.53%']\n",
      "Total recall: 85.08%\t| Recalls:\t['79.72%', '88.31%', '88.40%', '90.41%', '77.82%', '79.15%', '89.39%']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 09/10: 100%|██████████| 678/678 [03:51<00:00,  2.93it/s, loss=0.0028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race distribution: [1556. 1516. 1623. 1209. 1415. 1550. 2085.] Total: 10954\n",
      "Evaluation epoch 09/10:\n",
      "Total accuracy: 84.74%\t| Accuracies:\t['77.57%', '84.50%', '86.63%', '88.25%', '83.39%', '84.39%', '87.91%'] | Max disparity: 0.1290\n",
      "Total precision: 86.43%\t| Precisions:\t['78.92%', '83.55%', '85.64%', '93.40%', '87.31%', '86.69%', '88.70%']\n",
      "Total recall: 84.37%\t| Recalls:\t['76.85%', '85.66%', '87.26%', '88.81%', '79.59%', '81.34%', '88.86%']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 678/678 [03:53<00:00,  2.90it/s, loss=0.0006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race distribution: [1556. 1516. 1623. 1209. 1415. 1550. 2085.] Total: 10954\n",
      "Evaluation epoch 10/10:\n",
      "Total accuracy: 83.86%\t| Accuracies:\t['77.44%', '83.77%', '87.25%', '87.68%', '81.20%', '82.26%', '86.86%'] | Max disparity: 0.1241\n",
      "Total precision: 85.31%\t| Precisions:\t['78.28%', '81.33%', '85.73%', '90.79%', '86.02%', '85.45%', '88.41%']\n",
      "Total recall: 83.93%\t| Recalls:\t['77.60%', '87.38%', '88.65%', '90.90%', '76.19%', '77.86%', '86.99%']\n",
      "\n",
      "Total Training Time: 0:43:09\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Initiating experiment '{experiments[exp]}' with a lr of {lr} and {size_train} samples on device {device}\")\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {(epoch+1):02d}/{num_epochs:02d}\")\n",
    "    for _, (features, targets, _) in pbar:\n",
    "        \n",
    "        features = features.to(device)\n",
    "        targets = targets.float().to(device)\n",
    "            \n",
    "        # forward and backward pass\n",
    "        model_output = model(features)\n",
    "        logits = sigmoid(fc_layer(model_output))\n",
    "        loss = bin_ce(logits, targets)\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # update model params \n",
    "        optimizer.step()\n",
    "        \n",
    "        # if batch_idx == 0:\n",
    "        #     break \n",
    "\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False): # save memory during inference\n",
    "        acc_total, accs, max_acc_disp, prec_total, precs, rec_total, recs = evaluate_metrics(model, val_loader, device)\n",
    "        print(f\"Evaluation epoch {(epoch+1):02d}/{num_epochs:02d}:\")\n",
    "        print(f\"Total accuracy: {acc_total:.2%}\\t| Accuracies:\\t{accs} | Max disparity: {max_acc_disp:.4f}\")\n",
    "        print(f\"Total precision: {prec_total:.2%}\\t| Precisions:\\t{precs}\")\n",
    "        print(f\"Total recall: {rec_total:.2%}\\t| Recalls:\\t{recs}\\n\")\n",
    "    \n",
    "print(f\"Total Training Time: {get_elapsed_time(start_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation of experiment: 'CelebA only white'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 156/156 [00:27<00:00,  5.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race distribution: [ 1461.   553.  1269.  1538.   311.  1777. 13053.] Total: 19962\n",
      "\n",
      "Evaluation CelebA test set:\n",
      "Total accuracy: 98.05%\t| Accuracies:\t['95.28%', '98.92%', '99.29%', '98.50%', '92.93%', '96.90%', '98.43%']\n",
      "Maximum accuracy disparity: 0.0662\n",
      "Total precision: 98.07%\t| Precisions:\t['96.92%', '98.56%', '98.84%', '98.27%', '92.36%', '96.42%', '98.52%']\n",
      "Total recall: 96.86%\t| Recalls:\t['94.70%', '98.56%', '98.55%', '98.66%', '92.36%', '93.90%', '97.25%']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: 100%|██████████| 86/86 [00:18<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race distribution: [1556. 1516. 1623. 1209. 1415. 1550. 2085.] Total: 10954\n",
      "\n",
      "Evaluation FairFace test set:\n",
      "Total accuracy: 83.86%\t| Accuracies:\t['77.44%', '83.77%', '87.25%', '87.68%', '81.20%', '82.26%', '86.86%']\n",
      "Maximum accuracy disparity: 0.1241\n",
      "Total precision: 85.31%\t| Precisions:\t['78.28%', '81.33%', '85.73%', '90.79%', '86.02%', '85.45%', '88.41%']\n",
      "Total recall: 83.93%\t| Recalls:\t['77.60%', '87.38%', '88.65%', '90.90%', '76.19%', '77.86%', '86.99%']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate experiment on test sets\n",
    "with torch.set_grad_enabled(False): # save memory during inference\n",
    "    print(f\"\\nEvaluation of experiment: '{experiments[exp]}'\\n\")\n",
    "\n",
    "    # evaluation CelebA\n",
    "    acc_total, accs, max_acc_disp, prec_total, precs, rec_total, recs = evaluate_metrics(model, test_loader_celeb, device, show_tqdm=True)\n",
    "    print(\"\\nEvaluation CelebA test set:\")\n",
    "    print(f\"Total accuracy: {acc_total:.2%}\\t| Accuracies:\\t{accs}\")\n",
    "    print(f\"Maximum accuracy disparity: {max_acc_disp:.4f}\")\n",
    "    print(f\"Total precision: {prec_total:.2%}\\t| Precisions:\\t{precs}\")\n",
    "    print(f\"Total recall: {rec_total:.2%}\\t| Recalls:\\t{recs}\\n\")\n",
    "\n",
    "    # evaluation FairFace\n",
    "    acc_total, accs, max_acc_disp, prec_total, precs, rec_total, recs = evaluate_metrics(model, test_loader_ff, device, show_tqdm=True)\n",
    "    print(\"\\nEvaluation FairFace test set:\")\n",
    "    print(f\"Total accuracy: {acc_total:.2%}\\t| Accuracies:\\t{accs}\")\n",
    "    print(f\"Maximum accuracy disparity: {max_acc_disp:.4f}\")\n",
    "    print(f\"Total precision: {prec_total:.2%}\\t| Precisions:\\t{precs}\")\n",
    "    print(f\"Total recall: {rec_total:.2%}\\t| Recalls:\\t{recs}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
